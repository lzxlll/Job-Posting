{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cf763b97",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\DELL\\anaconda3\\lib\\site-packages\\scipy\\__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.2\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import openai\n",
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "from glob import glob\n",
    "from datetime import datetime\n",
    "import gc\n",
    "import requests\n",
    "import json\n",
    "import time\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "openai.organization = \"org-VerSGqrvW53HAZizo7yd3d6Z\"\n",
    "# openai.api_key = os.getenv(\"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\")\n",
    "openai.api_key = \"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\"\n",
    "# qdrant API key: OWelpamqQgfgsCfQLaRD0DTAtcjG7_LVIzj_N-eXySQ3VW-hsFuCNA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e915c27c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from qdrant_client import QdrantClient\n",
    "\n",
    "qdrant_client = QdrantClient(\n",
    "    host=\"696cd6bf-93a7-45ce-9672-f02a1ddd98f0.us-east-1-0.aws.cloud.qdrant.io\", \n",
    "    api_key=\"OWelpamqQgfgsCfQLaRD0DTAtcjG7_LVIzj_N-eXySQ3VW-hsFuCNA\",\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f28d7eb8",
   "metadata": {},
   "source": [
    "### We first load a single dataframe generated from 'data_purge_v1', in which includes all the job postings with title and IDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8921428",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('E:/Data/job_posting/processed/estimation/charac_posting.csv', encoding = \"utf_8_sig\", on_bad_lines='skip', usecols = ['工作名称'])\n",
    "df.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8b9ea36e",
   "metadata": {},
   "source": [
    "### Then, we clean this dataframe by filtering out the job posting titles with duplicates less than 5 times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc13884d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# count the occurrences of each unique value in the 工作名称 column of a DataFrame df. The result is stored in a new variable called df_counted\n",
    "df_counted = df['工作名称'].value_counts()\n",
    "\n",
    "# Among the 99,315,582 total job postings, there is 35,537,095 job posting titles. \n",
    "df_filtered = df_counted[df_counted>5]\n",
    "df_filtered.shape\n",
    "\n",
    "# Filter to value > 5. This means we need to use ChatGPT to parse 883,695 job posting titles: this is equal to 50,340,840 total job postings\n",
    "df_filtered.sum()\n",
    "\n",
    "# convert 'df_filtered' to a dataframe including a title column and a count column\n",
    "df_filtered = df_filtered.to_frame().reset_index()\n",
    "df_filtered.columns = ['工作名称', 'count']\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ce594642",
   "metadata": {},
   "source": [
    "### Parallelize the job title classification using Python's ThreadPoolExecutor, we feed the job titles to ChatGPT to map it to a SOC category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38e1c92c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataframe into 1000 sub-dataframes based on index\n",
    "sub_dfs = np.array_split(df_filtered, 300)\n",
    "\n",
    "# Export each sub-dataframe to a CSV file\n",
    "for i, sub_df in enumerate(sub_dfs):\n",
    "    sub_df.to_csv(f'E:/Data/job_posting/processed/title_raw/title_{i+1}.csv', index=True, encoding = \"utf_8_sig\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c514bfef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_job_title(job_title, api_key):\n",
    "    url = 'https://api.openai.com/v1/chat/completions'\n",
    "    headers = {'Content-Type': 'application/json',\n",
    "               'Authorization': f'Bearer {api_key}'}\n",
    "    data = {'model': 'gpt-3.5-turbo-0301',\n",
    "            'messages':[\n",
    "                {\n",
    "                'role': 'user', \n",
    "                'content': f'The most likely Standard Occupational Classification title and code the occupation fall into: \"{job_title}\", only show the SOC code.'\n",
    "                }\n",
    "                       ]\n",
    "            }\n",
    "    try:\n",
    "        response = requests.post(url, headers=headers, json=data, verify=False)\n",
    "        response_data = json.loads(response.text)\n",
    "        if response.status_code != 200:\n",
    "            raise Exception(response_data['error']['message'])\n",
    "        return response_data['choices'][0]['message']['content']\n",
    "    except Exception as e:\n",
    "        print(f'Error occurred: {e}')\n",
    "        return 'N/A'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1400679",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parallelize the job title classification using Python's ThreadPoolExecutor\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import requests\n",
    "import json\n",
    "import urllib3\n",
    "\n",
    "# Disable SSL warnings\n",
    "urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\n",
    "\n",
    "os.environ[\"http_proxy\"] = \"http://127.0.0.1:10809\"\n",
    "os.environ[\"https_proxy\"] = \"http://127.0.0.1:10809\"\n",
    "api_key = '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'\n",
    "\n",
    "# create a thread pool with 30 worker threads\n",
    "with ThreadPoolExecutor(max_workers=30) as executor:\n",
    "    # iterate over the file names\n",
    "    for i in range(1, 301):\n",
    "        # read the file into a dataframe\n",
    "        filename = f'E:/Data/job_posting/processed/title_raw/title_{i}.csv'\n",
    "        df = pd.read_csv(filename, encoding=\"utf_8_sig\", on_bad_lines='skip')\n",
    "\n",
    "        # extract the job titles and submit them to the thread pool\n",
    "        job_titles = df['工作名称'].tolist()\n",
    "        futures = [executor.submit(classify_job_title, job_title, api_key) for job_title in job_titles]\n",
    "\n",
    "        # wait for all threads to complete and get the results\n",
    "        # create an empty list to store soc codes\n",
    "        soc_codes = []\n",
    "        for future in futures:\n",
    "            soc_code = future.result()\n",
    "            soc_codes.append(soc_code)\n",
    "\n",
    "        # append the soc codes to the dataframe\n",
    "        df['soc_code'] = soc_codes\n",
    "        # clean the soc codes\n",
    "        df['soc_code'] = df['soc_code'].str.extract(r'(\\d{2}-\\d{4})')\n",
    "        df.to_csv(f'E:/Data/job_posting/processed/title_mapped/title_{i}.csv', encoding=\"utf_8_sig\") \n",
    "        del df\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d95aa629",
   "metadata": {},
   "source": [
    "### Append all the csv files under folder `E:/Data/job_posting/processed/title_mapped` to a single dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5d79555",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Append all the csv files under folder `E:/Data/job_posting/processed/title_mapped` to a single dataframe\n",
    "path = r'F:/Data/job_posting/processed/title_mapped' # use your path\n",
    "all_files = glob(os.path.join(path, \"*.csv\"))\n",
    "# Append all the csv files in 'all_files' to a single dataframe\n",
    "df_from_each_file = (pd.read_csv(f, encoding=\"utf_8_sig\") for f in all_files)\n",
    "df_title = pd.concat(df_from_each_file, ignore_index=True)\n",
    "# subset dataframe 'df_title' to only keep '工作名称', 'soc_code'\n",
    "df_title = df_title[['工作名称', 'soc_code']]\n",
    "# drop the missing value in column 'soc_code'\n",
    "df_title = df_title.dropna(subset=['soc_code'])\n",
    "df_title"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7e71c888",
   "metadata": {},
   "source": [
    "### The next step is to map the rest unmapped job postings to the SOCs using the job descriptions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c227256e",
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = 'F:/Data/job_posting/mapped_job_posting/Update file/'\n",
    "\n",
    "# create an empty DataFrame to store merged data\n",
    "df_titleLabel = pd.DataFrame()\n",
    "\n",
    "# iterate over files in that directory\n",
    "for filename in os.listdir(directory):\n",
    "    # checking if it is a file\n",
    "    if filename.startswith(\"job_res_\"): # for files start with a prefix #\n",
    "        f = os.path.join(directory, filename)\n",
    "        df = pd.read_csv(f, encoding = \"utf_8_sig\", on_bad_lines='skip', delimiter= \"?\", encoding_errors='ignore')\n",
    "        df.rename(columns={'招聘ID': '招聘主键ID'}, inplace=True)\n",
    "        df = df[['招聘主键ID', '工作描述', '工作名称']]\n",
    "    # merge 'df_title' with 'df', based on column '工作名称'. Keep the matched row in csv file 'E:/Data/job_posting/processed/finetune/'\n",
    "        df_titleData = pd.merge(df, df_title, on='工作名称', how='inner')# append the merged DataFrame to the empty DataFrame\n",
    "        # change data type of 'soc_code' column to string\n",
    "        # df_titleLabel['soc_code'] = df_titleLabel['soc_code'].astype(str)\n",
    "        df_titleLabel = df_titleLabel.append(df_titleData)\n",
    "        df_titleLabel['soc_code'] = df_titleLabel['soc_code'].astype(str)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2c9e4e8d",
   "metadata": {},
   "source": [
    "### To do this, we first load the dataframe from ONET which contains all the possible SOC job titles. This step helps to remove incorrect mapping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c00e195d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the ONET SOC Code (the real one)\n",
    "df_soc = pd.read_csv('F:/Data/job_posting/processed/2019_to_SOC_Crosswalk.csv')\n",
    "# keep column '2018 SOC Code'\n",
    "df_soc = df_soc[['2018 SOC Code']]\n",
    "# replace the last digit of 'soc_code' with '0'\n",
    "df_soc['2018 SOC Code'] = df_soc['2018 SOC Code'].str[:-1] + '0'\n",
    "# rename the column name to 'soc_code'\n",
    "df_soc.rename(columns={'2018 SOC Code': 'soc_code'}, inplace=True)\n",
    "df_soc = df_soc.drop_duplicates(subset=['soc_code'], keep='first')\n",
    "len(df_soc['soc_code'].unique())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cb6e9d12",
   "metadata": {},
   "source": [
    "### We set the finest level to 6 digits, which already covers 459 broad occupations. The concern of going into more granular level is that the number of job postings will be too small to train a good model.\n",
    "\n",
    "- However, not all the broad occupations show up equally in the dataset. We filter out the broad occupations with less than 100 job postings. In this way, it has enough observation to train a good model.\n",
    "- We end up with 408 broad occupations.\n",
    "- we randonmly sample 3000 job postings within each broad occupations, and save them to csv files. We feed this data to ChatGPT to map the job postings to the SOC categories by using job descriptions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ece54c48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace the last digit of 'soc_code' with '0'\n",
    "df_titlelabel['soc_code'] = df_titlelabel['soc_code'].str[:-1] + '0'\n",
    "# merge the 'test_dfSoc' and 'df_soc' using 'soc_code', only keep the matched sample\n",
    "df_titlelabel = pd.merge(df_titlelabel, df_soc, on='soc_code', how='inner')\n",
    "# keep number of observations by 'soc_code' is more than 100\n",
    "df_titlelabel = df_titlelabel.groupby('soc_code').filter(lambda x: len(x) > 100)\n",
    "# save the final merged DataFrame to a csv file\n",
    "df_titlelabel.to_csv('F:/Data/job_posting/processed/finetune/df_titleLabel.csv', index=False, encoding = \"utf_8_sig\", header=True, quoting=csv.QUOTE_NONNUMERIC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "971b39b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "# randomly select 3000 samples within each unique value of 'soc_code'\n",
    "df = df_titlelabel.groupby('soc_code', group_keys=False).apply(lambda x: x.sample(min(len(x), 3000)))\n",
    "df.to_csv('F:/Data/job_posting/processed/finetune/secondcheck_sample.csv', index=False, encoding = \"utf_8_sig\", header=True, quoting=csv.QUOTE_NONNUMERIC)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c7692fa2",
   "metadata": {},
   "source": [
    "### Afer first labelling based on the title of job posting, we again use GPT to verify the labelling based on the description of job posting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0cb9aaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_job_desp(desp, job_title, api_key):\n",
    "    url = 'https://api.openai.com/v1/chat/completions'\n",
    "    headers = {'Content-Type': 'application/json',\n",
    "               'Authorization': f'Bearer {api_key}'}\n",
    "    data = {'model': 'gpt-3.5-turbo-0301',\n",
    "            'messages':[\n",
    "                {\n",
    "                'role': 'user', \n",
    "                'content': f\"Based on this job description (in Chinese): '{desp}' Is this Standard Occupational Classification code: '{job_title}' a reasonable classification (at broad group level)? Only tell me yes or no.\"\n",
    "                }\n",
    "                       ]\n",
    "            }\n",
    "    try:\n",
    "        response = requests.post(url, headers=headers, json=data, verify=False)\n",
    "        response_data = json.loads(response.text)\n",
    "        if response.status_code != 200:\n",
    "            raise Exception(response_data['error']['message'])\n",
    "        return response_data['choices'][0]['message']['content']\n",
    "    except Exception as e:\n",
    "        print(f'Error occurred: {e}')\n",
    "        return 'N/A'\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0683b1c4",
   "metadata": {},
   "source": [
    "### Double check on the sub-sampled dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3defde16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parallelize the job title classification using Python's ThreadPoolExecutor\n",
    "import os\n",
    "import pandas as pd\n",
    "import requests\n",
    "import json\n",
    "import urllib3\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "# Disable SSL warnings\n",
    "urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\n",
    "\n",
    "os.environ[\"http_proxy\"] = \"http://127.0.0.1:10809\"\n",
    "os.environ[\"https_proxy\"] = \"http://127.0.0.1:10809\"\n",
    "api_key = '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'\n",
    "\n",
    "# Read the file into a dataframe, start from the 3000th row\n",
    "df = pd.read_csv('F:/Data/job_posting/processed/finetune/secondcheck_sample.csv', encoding = \"utf_8_sig\", on_bad_lines='skip', encoding_errors='ignore')\n",
    "\n",
    "# Create a list of tuples containing (desp, job_title) pairs\n",
    "desps = df['工作描述'].tolist()\n",
    "job_titles = df['soc_code'].tolist()\n",
    "desp_job_title_pairs = list(zip(desps, job_titles))\n",
    "\n",
    "# Function to handle ThreadPoolExecutor map\n",
    "def classify_wrapper(args):\n",
    "    return classify_job_desp(*args)\n",
    "\n",
    "# Create a thread pool with 30 worker threads\n",
    "with ThreadPoolExecutor(max_workers=20) as executor:\n",
    "    # Submit desp_job_title_pairs to the thread pool\n",
    "    true_inds = list(executor.map(classify_wrapper, [(desp, job_title, api_key) for desp, job_title in desp_job_title_pairs]))\n",
    "\n",
    "# Append the soc_codes to the dataframe\n",
    "df['true_ind'] = true_inds\n",
    "# Clean the soc_codes\n",
    "# remove '.' from 'true_ind'\n",
    "df['true_ind'] = df['true_ind'].str.replace('.', '')\n",
    "df.to_csv('F:/Data/job_posting/processed/finetune/secondcheck_sample_ind.csv', encoding=\"utf_8_sig\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ea690b41",
   "metadata": {},
   "source": [
    "### Generate the final dataset for model finetune\n",
    "\n",
    "- The first screen is filtered based on the job title, and we left with 32 million-ish job postings.\n",
    "- The second screen is based on the job description. From the 32 million-ish job postings, we randomly sample 3000 observations within each unique SOC category and feed them to GPT for the second check.\n",
    "- We keep the samples that pass the second check, which means the SOC category predicted by GPT is the same as the SOC category predicted by the job title. This is the first part of the final dataset.\n",
    "- The second part of the final dataset is the job postings that are first screened by the job title, but has not been selected by the random sampling for the second screen. Within this data pool, we randomly sample 2000 observations within each unique SOC category. This is the second part of the final dataset.\n",
    "- We combine the two parts of the final dataset, and we assign weight 1 for the second check sample and assign weight 0.5 for the random sample. This is the final dataset for model finetune."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2feb6e17",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "df_sample = pd.read_csv('F:/Data/job_posting/processed/finetune/secondcheck_sample_ind.csv', encoding=\"utf_8_sig\")\n",
    "\n",
    "# keep if true_ind is 'Yes'\n",
    "df_sample = df_sample[df_sample['true_ind'] == 'Yes']\n",
    "\n",
    "# create into train and test set\n",
    "df_titlelabel = pd.read_csv('F:/Data/job_posting/processed/finetune/df_titleLabel.csv', encoding = \"utf_8_sig\", on_bad_lines='skip', encoding_errors='ignore')\n",
    "\n",
    "# merge the dataframes on '招聘主键ID'\n",
    "merged_df = df_sample.merge(df_titlelabel, on='招聘主键ID', how='outer', indicator=True)\n",
    "\n",
    "# filter the merged dataframe to keep only the observations in 'df_titlelabel' but not in 'df_sample'\n",
    "# merged_df.reset_index(inplace=True)\n",
    "filtered_df = merged_df.loc[merged_df['_merge'] == 'right_only', ['招聘主键ID', '工作描述_y', '工作名称_y', 'soc_code_y']]\n",
    "\n",
    "# note that df_titlelabel.columns is used to select only the columns from 'df_titlelabel'\n",
    "# rename columns by removing '_y'\n",
    "filtered_df = filtered_df.rename(columns={'工作描述_y': '工作描述', '工作名称_y': '工作名称', 'soc_code_y': 'soc_code'})\n",
    "\n",
    "# drop if '描述' is null\n",
    "filtered_df = filtered_df.dropna(subset=['工作描述'])\n",
    "\n",
    "# randomly select 3000 samples within each unique value of 'soc_code'\n",
    "filtered_df = filtered_df.groupby('soc_code', group_keys=False).apply(lambda x: x.sample(min(len(x), 5000)))\n",
    "\n",
    "# append 'df_sample' and 'filtered_df', create a new dataframe 'df'\n",
    "df = pd.concat([df_sample, filtered_df], axis = 0)\n",
    "\n",
    "# drop if the first two digits in 'soc_code' are '55'\n",
    "df = df[~df['soc_code'].str.startswith('55')]\n",
    "\n",
    "df.to_csv('F:/Data/job_posting/processed/finetune/est_sample.csv', index=False, encoding = \"utf_8_sig\", header=True, quoting=csv.QUOTE_NONNUMERIC)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d4453c50",
   "metadata": {},
   "source": [
    "\n",
    "- Below are the test code, do not use for estimation !!!\n",
    "\n",
    "- Below are the test code, do not use for estimation !!!\n",
    "\n",
    "- Below are the test code, do not use for estimation !!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b9ebddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: you need to be using OpenAI Python v0.27.0 for the code below to work\n",
    "\n",
    "os.environ[\"http_proxy\"] = \"http://127.0.0.1:10809\"\n",
    "os.environ[\"https_proxy\"] = \"http://127.0.0.1:10809\"\n",
    "\n",
    "response = openai.ChatCompletion.create(\n",
    "  model=\"gpt-3.5-turbo\",\n",
    "  messages=[\n",
    "        {\"role\": \"user\", \"content\": \"\"\"\n",
    "          Q: The most likely Standard Occupational Classification's title and code this occupation fall into: 自媒体, only show the SOC code.\n",
    "          A:\"\"\"}\n",
    "    ],\n",
    "  top_p = 0.3\n",
    ")\n",
    "\n",
    "res = response['choices'][0]['message']['content']\n",
    "# only keep the soc code\n",
    "# res = re.findall(r'(\\d{2}-\\d{4})', res)\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75ef7d50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: you need to be using OpenAI Python v0.27.0 for the code below to work\n",
    "\n",
    "os.environ[\"http_proxy\"] = \"http://127.0.0.1:10809\"\n",
    "os.environ[\"https_proxy\"] = \"http://127.0.0.1:10809\"\n",
    "\n",
    "response = openai.ChatCompletion.create(\n",
    "  model=\"gpt-3.5-turbo\",\n",
    "  messages=[\n",
    "        {\"role\": \"user\", \"content\": \"\"\"\n",
    "          Q: The most likely Standard Occupational Classification's title and code this occupation fall into: 自媒体, only show the SOC code.\n",
    "          A:\"\"\"}\n",
    "    ],\n",
    "  top_p = 0.3\n",
    ")\n",
    "\n",
    "res = response['choices'][0]['message']['content']\n",
    "# only keep the soc code\n",
    "# res = re.findall(r'(\\d{2}-\\d{4})', res)\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50487eeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: you need to be using OpenAI Python v0.27.0 for the code below to work\n",
    "response = openai.ChatCompletion.create(\n",
    "  model=\"gpt-3.5-turbo\",\n",
    "  messages=[\n",
    "        {\"role\": \"user\", \"content\": \"translate into Chinese: Collect water and soil samples to test for physical, chemical, or biological properties, such as pH, oxygen level, temperature, and pollution\"}\n",
    "    ]\n",
    ")\n",
    "\n",
    "res = response['choices'][0]['message']['content']\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cda7f84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Among the 130,646,620 total job postings, there is 42,237,934 job posting titles. Filter to values > 2, this is doable\n",
    "df_filtered = df_counted[df_counted>100]\n",
    "df_filtered.shape\n",
    "df_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1323135f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\DELL\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3444: DtypeWarning: Columns (5) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>招聘主键ID</th>\n",
       "      <th>工作描述</th>\n",
       "      <th>工作名称</th>\n",
       "      <th>soc_code</th>\n",
       "      <th>true_ind</th>\n",
       "      <th>soc_code1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>411683</th>\n",
       "      <td>NaN</td>\n",
       "      <td>107551668</td>\n",
       "      <td>工作内容： 1.负责公司网络营销管理，统筹公司电商部门的各项工作； 2.推动公司线上营销业务...</td>\n",
       "      <td>电商运营总监/经理</td>\n",
       "      <td>111020</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>418579</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1776000</td>\n",
       "      <td>1、负责公司保密制度的建设和管理；2、负责公司管理制度的建设和执行；3、负责档案管理；4、负...</td>\n",
       "      <td>行政管理</td>\n",
       "      <td>113010</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>421725</th>\n",
       "      <td>NaN</td>\n",
       "      <td>150440510</td>\n",
       "      <td>职位描述：1、主导技术难题攻关，持续提升核心系统在高并发、海量请求数下的高处理性能，解决各类...</td>\n",
       "      <td>技术总监cto</td>\n",
       "      <td>113020</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>426022</th>\n",
       "      <td>NaN</td>\n",
       "      <td>113371814</td>\n",
       "      <td>1. 负责与加工厂计划沟通与协调；2. 跟进物料和生产进度；3. 物料计划实施与控制；4. ...</td>\n",
       "      <td>PMC生产计划</td>\n",
       "      <td>113050</td>\n",
       "      <td>False</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>429997</th>\n",
       "      <td>NaN</td>\n",
       "      <td>65548415</td>\n",
       "      <td>岗位职责：1、执行物资管理中与仓库有关的SOP，确保仓库作业顺利进行；2、负责仓库日常物资的...</td>\n",
       "      <td>仓库管理员4500</td>\n",
       "      <td>113070</td>\n",
       "      <td>False</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>261568</th>\n",
       "      <td>608485.0</td>\n",
       "      <td>74307604</td>\n",
       "      <td>1、大专及以上学历，英语四级；2、有船务工作经验；3、沟通能力强。</td>\n",
       "      <td>船务</td>\n",
       "      <td>535020</td>\n",
       "      <td>True</td>\n",
       "      <td>141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>261806</th>\n",
       "      <td>609077.0</td>\n",
       "      <td>18761610</td>\n",
       "      <td>岗位职责:1、负责进出口业务的操作，安排订舱、运输、装箱、货物跟踪、报关报检、保险等海运流程...</td>\n",
       "      <td>海运操作主管</td>\n",
       "      <td>535020</td>\n",
       "      <td>True</td>\n",
       "      <td>141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>989492</th>\n",
       "      <td>NaN</td>\n",
       "      <td>118875853</td>\n",
       "      <td>岗位要求 1.女性，大专以上学历，身高165cm以上，形象气质佳 2.汽车服务行业经验者优先...</td>\n",
       "      <td>引导员</td>\n",
       "      <td>536020</td>\n",
       "      <td>False</td>\n",
       "      <td>142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>404388</th>\n",
       "      <td>303755.0</td>\n",
       "      <td>116090313</td>\n",
       "      <td>岗位职责： 1、接收订单以及安排订舱、运输、货物跟踪、报关、结算等事宜； 2、根据客户提供的...</td>\n",
       "      <td>水/空/陆运操作</td>\n",
       "      <td>537010</td>\n",
       "      <td>True</td>\n",
       "      <td>143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1006144</th>\n",
       "      <td>NaN</td>\n",
       "      <td>100736128</td>\n",
       "      <td>岗位职责：1、根据先进先出原则收发货物。2、根据转仓单和装运单签名发货并核对。3、每天应对铲...</td>\n",
       "      <td>叉车司机</td>\n",
       "      <td>537050</td>\n",
       "      <td>False</td>\n",
       "      <td>144</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>200 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Unnamed: 0     招聘主键ID  \\\n",
       "411683          NaN  107551668   \n",
       "418579          NaN    1776000   \n",
       "421725          NaN  150440510   \n",
       "426022          NaN  113371814   \n",
       "429997          NaN   65548415   \n",
       "...             ...        ...   \n",
       "261568     608485.0   74307604   \n",
       "261806     609077.0   18761610   \n",
       "989492          NaN  118875853   \n",
       "404388     303755.0  116090313   \n",
       "1006144         NaN  100736128   \n",
       "\n",
       "                                                      工作描述       工作名称  \\\n",
       "411683   工作内容： 1.负责公司网络营销管理，统筹公司电商部门的各项工作； 2.推动公司线上营销业务...  电商运营总监/经理   \n",
       "418579   1、负责公司保密制度的建设和管理；2、负责公司管理制度的建设和执行；3、负责档案管理；4、负...       行政管理   \n",
       "421725   职位描述：1、主导技术难题攻关，持续提升核心系统在高并发、海量请求数下的高处理性能，解决各类...    技术总监cto   \n",
       "426022   1. 负责与加工厂计划沟通与协调；2. 跟进物料和生产进度；3. 物料计划实施与控制；4. ...    PMC生产计划   \n",
       "429997   岗位职责：1、执行物资管理中与仓库有关的SOP，确保仓库作业顺利进行；2、负责仓库日常物资的...  仓库管理员4500   \n",
       "...                                                    ...        ...   \n",
       "261568                   1、大专及以上学历，英语四级；2、有船务工作经验；3、沟通能力强。         船务   \n",
       "261806   岗位职责:1、负责进出口业务的操作，安排订舱、运输、装箱、货物跟踪、报关报检、保险等海运流程...     海运操作主管   \n",
       "989492   岗位要求 1.女性，大专以上学历，身高165cm以上，形象气质佳 2.汽车服务行业经验者优先...        引导员   \n",
       "404388   岗位职责： 1、接收订单以及安排订舱、运输、货物跟踪、报关、结算等事宜； 2、根据客户提供的...   水/空/陆运操作   \n",
       "1006144  岗位职责：1、根据先进先出原则收发货物。2、根据转仓单和装运单签名发货并核对。3、每天应对铲...       叉车司机   \n",
       "\n",
       "        soc_code  true_ind  soc_code1  \n",
       "411683    111020     False          0  \n",
       "418579    113010     False          1  \n",
       "421725    113020     False          2  \n",
       "426022    113050     False          3  \n",
       "429997    113070     False          4  \n",
       "...          ...       ...        ...  \n",
       "261568    535020      True        141  \n",
       "261806    535020      True        141  \n",
       "989492    536020     False        142  \n",
       "404388    537010      True        143  \n",
       "1006144   537050     False        144  \n",
       "\n",
       "[200 rows x 7 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, AdamW, AutoModelForMaskedLM\n",
    "from transformers import BertForSequenceClassification\n",
    "import torch\n",
    "from transformers import AdamW\n",
    "from transformers import get_scheduler\n",
    "from transformers import Trainer\n",
    "from transformers import BertTokenizer\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "\n",
    "\n",
    "#tokenizer = AutoTokenizer.from_pretrained(\"bert-base-chinese\")\n",
    "tokenizer = BertTokenizer.from_pretrained(\"G:/Other computers/我的计算机/cloud_share/Job_posting_data/chinese-bert-wwm/\")\n",
    "\n",
    "df = pd.read_csv(\"F:/Data/job_posting/processed/finetune/est_sample.csv\", encoding = \"utf_8_sig\", on_bad_lines='skip', encoding_errors='ignore')\n",
    "df = df.sample(n=200, random_state=1)\n",
    "\n",
    "# replace the symbol '-' to '.' in soc_code column, and convert soc_code to int\n",
    "df['soc_code'] = df['soc_code'].str.replace('-', '')\n",
    "\n",
    "# generate a new column 'soc_code1' with value to recode the 'soc_code' in ascending order\n",
    "df['soc_code1'] = df['soc_code'].rank(method='dense').astype(int) - 1\n",
    "\n",
    "# replace 'Yes' with True and NaN with False using the fillna() and astype() methods\n",
    "df['true_ind'] = df['true_ind'].fillna(False).astype(bool)\n",
    "\n",
    "# sort by soc_code1\n",
    "df = df.sort_values(by=['soc_code1'])\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "6dcbdfc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\DELL\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3444: DtypeWarning: Columns (5) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "Some weights of the model checkpoint at G:/Other computers/我的计算机/cloud_share/Job_posting_data/chinese-bert-wwm/ were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at G:/Other computers/我的计算机/cloud_share/Job_posting_data/chinese-bert-wwm/ and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "c:\\Users\\DELL\\anaconda3\\lib\\site-packages\\transformers\\optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4, Loss: 3.5583\n",
      "Epoch 2/4, Loss: 3.4560\n",
      "Epoch 3/4, Loss: 3.3844\n",
      "Epoch 4/4, Loss: 3.2918\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, AdamW, AutoModelForMaskedLM\n",
    "from transformers import BertForSequenceClassification\n",
    "import torch\n",
    "from transformers import AdamW\n",
    "from transformers import get_scheduler\n",
    "from transformers import Trainer\n",
    "from transformers import BertTokenizer\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#tokenizer = AutoTokenizer.from_pretrained(\"bert-base-chinese\")\n",
    "tokenizer = BertTokenizer.from_pretrained(\"G:/Other computers/我的计算机/cloud_share/Job_posting_data/chinese-bert-wwm/\")\n",
    "\n",
    "df = pd.read_csv(\"F:/Data/job_posting/processed/finetune/est_sample.csv\", encoding = \"utf_8_sig\", on_bad_lines='skip', encoding_errors='ignore')\n",
    "df = df.sample(n=200, random_state=1)\n",
    "\n",
    "# replace the symbol '-' to '.' in soc_code column, and convert soc_code to int\n",
    "df['soc_code'] = df['soc_code'].str.replace('-', '')\n",
    "\n",
    "# replace 'Yes' with True and NaN with False using the fillna() and astype() methods\n",
    "df['true_ind'] = df['true_ind'].fillna(False).astype(bool)\n",
    "\n",
    "\n",
    "\n",
    "# create into train and test set\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_df_sample, test_df_sample = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "# drop index column, 'true_ind' and 'sample' columns\n",
    "train_df_sample = train_df_sample.drop(['Unnamed: 0'], axis = 1)\n",
    "# generate a new column 'soc_code1' with value to recode the 'soc_code' in ascending order\n",
    "train_df_sample['soc_code1'] = train_df_sample['soc_code'].rank(method='dense').astype(int) - 1\n",
    "# drop index column, 'true_ind' and 'sample' columns\n",
    "test_df_sample = test_df_sample.drop(['Unnamed: 0'], axis = 1)\n",
    "# generate a new column 'soc_code1' with value to recode the 'soc_code' in ascending order\n",
    "test_df_sample['soc_code1'] = test_df_sample['soc_code'].rank(method='dense').astype(int) - 1\n",
    "\n",
    "# convert '工作描述' to string\n",
    "train_df_sample['工作描述'] = train_df_sample['工作描述'].astype(str)\n",
    "test_df_sample['工作描述'] = test_df_sample['工作描述'].astype(str)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Tokenize the text and convert it into input features\n",
    "train_texts = train_df_sample['工作描述'].tolist()\n",
    "train_labels = train_df_sample['soc_code1'].tolist()\n",
    "train_encodings = tokenizer(train_texts, truncation=True, padding=True, max_length=512)\n",
    "\n",
    "test_texts = test_df_sample['工作描述'].tolist()\n",
    "test_labels = test_df_sample['soc_code1'].tolist()\n",
    "test_encodings = tokenizer(test_texts, truncation=True, padding=True, max_length=512)\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from transformers import BertForSequenceClassification, AdamW, get_linear_schedule_with_warmup\n",
    "\n",
    "# Convert the input features into PyTorch tensors\n",
    "train_inputs = torch.tensor(train_encodings['input_ids'])\n",
    "train_masks = torch.tensor(train_encodings['attention_mask'])\n",
    "train_labels = torch.tensor(train_labels)\n",
    "\n",
    "test_inputs = torch.tensor(test_encodings['input_ids'])\n",
    "test_masks = torch.tensor(test_encodings['attention_mask'])\n",
    "test_labels = torch.tensor(test_labels)\n",
    "\n",
    "def assign_weight(true_ind):\n",
    "    if true_ind:\n",
    "        return 1.0\n",
    "    else:\n",
    "        return 0.5\n",
    "\n",
    "# Assuming you have a column called 'true_ind' in your dataset with boolean values\n",
    "# True for credible labels and False for less credible labels\n",
    "train_df_sample['weight'] = train_df_sample['true_ind'].apply(assign_weight)\n",
    "test_df_sample['weight'] = test_df_sample['true_ind'].apply(assign_weight)\n",
    "\n",
    "# Convert the weights column to PyTorch tensors\n",
    "train_weights = torch.tensor(train_df_sample['weight'].tolist())\n",
    "test_weights = torch.tensor(test_df_sample['weight'].tolist())\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from transformers import BertForSequenceClassification, AdamW, get_linear_schedule_with_warmup\n",
    "\n",
    "# Create a PyTorch DataLoader to iterate over the data during training\n",
    "batch_size = 5\n",
    "\n",
    "train_data = TensorDataset(train_inputs, train_masks, train_labels, train_weights)\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "test_data = TensorDataset(test_inputs, test_masks, test_labels, test_weights)\n",
    "test_loader = DataLoader(test_data, batch_size=batch_size)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Define the model, the optimizer, and the learning rate scheduler\n",
    "num_labels = len(train_df_sample['soc_code1'].unique())\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained(\"G:/Other computers/我的计算机/cloud_share/Job_posting_data/chinese-bert-wwm/\", num_labels=num_labels)\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5, eps=1e-8)\n",
    "num_epochs = 4\n",
    "num_training_steps = num_epochs * len(train_loader)\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=num_training_steps)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "unique_labels = np.arange(torch.max(train_labels).item() + 1)\n",
    "class_weights = compute_class_weight('balanced', classes=unique_labels, y=train_labels.numpy())\n",
    "class_weights = torch.tensor(class_weights, dtype=torch.float)\n",
    "\n",
    "# Train the model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Initialize lists to store losses\n",
    "train_losses = []\n",
    "\n",
    "# Utilize multiple GPUs with DataParallel\n",
    "if torch.cuda.device_count() > 1:\n",
    "    model = torch.nn.DataParallel(model)\n",
    "\n",
    "# Create a loss function that doesn't reduce the losses right away and pass class_weights\n",
    "loss_fn = torch.nn.CrossEntropyLoss(weight=class_weights.to(device), reduction='none')\n",
    "\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_train_loss = 0\n",
    "    num_batches = 0\n",
    "    for batch in train_loader:\n",
    "        inputs = batch[0].to(device)\n",
    "        masks = batch[1].to(device)\n",
    "        labels = batch[2].to(device)\n",
    "        weights = batch[3].to(device)  # Assuming the weights are the 4th element in the batch\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        logits = model(inputs, attention_mask=masks).logits\n",
    "\n",
    "        # Compute the loss for each sample\n",
    "        batch_loss = loss_fn(logits, labels)\n",
    "\n",
    "        # Multiply the loss by the corresponding weight\n",
    "        weighted_batch_loss = batch_loss * weights\n",
    "\n",
    "        # Average the weighted losses\n",
    "        loss = torch.mean(weighted_batch_loss)\n",
    "\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Add the current batch loss to the epoch_train_loss\n",
    "        epoch_train_loss += loss.item()\n",
    "        num_batches += 1\n",
    "\n",
    "    # Calculate average loss for the current epoch and append it to the train_losses list\n",
    "    epoch_train_loss /= num_batches\n",
    "    train_losses.append(epoch_train_loss)\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {epoch_train_loss:.4f}\")\n",
    "\n",
    "# Save the major model\n",
    "if isinstance(model, torch.nn.DataParallel):\n",
    "    model.module.save_pretrained(\"F:/Data/job_posting/processed/finetune/plain_model\")\n",
    "else:\n",
    "    model.save_pretrained(\"F:/Data/job_posting/processed/finetune/plain_model\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "531c072d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
