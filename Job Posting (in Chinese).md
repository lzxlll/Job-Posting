# Job-Posting

原始招聘数据共有 140,107,287 条观测记录。

本项目旨在：
1. 清洗原始数据。
2. 为微调的数据样本打标签。
3. 微调 BERT-WWM 模型。
4. 评估模型性能。

## 数据概述

| 变量名 | 格式 | 变量名 | 格式 | 变量名 | 格式 |
| --- | --- |  --- | --- |   --- | --- |
|招聘主键ID  | bigint(20)| 工作薪酬 | varchar(255)| 工作名称 | varchar(255)|
|公司ID  | bigint(20)| 教育要求 | varchar(255)| 招聘数量 | varchar(255)|
|公司名称 | varchar(255)| 工作经历 | varchar(255)|发布日期  | datetime|
|城市名称 | varchar(255)| 工作描述 | varchar(255)|行业名称 | varchar(255)|
|公司所在区域 | varchar(255)| 职位名称 | varchar(255)|来源 | varchar(255)|


## ***数据清洗概述***

在本部分中，我们概述了用于分析的招聘数据生成和清洗过程。主要数据来源于中国10个主要招聘网站。通过清洗数据去除重复、不必要的条目和不一致项，以确保分析的可靠性和准确性。

#### 1. 数据导入和初步清洗
使用 `read_dat_data` 函数将原始数据文件导入为 pandas DataFrames。此函数读取 `.dat` 文件，并为 DataFrames 分配适当的列名。

#### 2. 数据预处理
- 将 `工作名称`（职位名称）列中的缺失值替换为 `职位名称` 列中的值。
- 删除缺少 `发布日期`（发布日期）的行。
- 删除职位名称中包含 `兼职`（part-time）条目的记录。
- 数据集过滤为仅包含来自中国10个主要招聘网站的数据。
- 删除一个月内相同 `公司ID`（公司ID）、`工作名称`（职位名称）和 `城市名称`（城市名称）的重复条目。

#### 3. 数据分离
在预处理之后，将 `工作描述`（工作描述）列从其余数据中分离出来，以便于数据集的管理。

#### 4. 数据聚合
清洗和预处理后的数据保存为单独的 CSV 文件。所有 CSV 文件合并为一个包含 `工作名称`（职位名称）列的 DataFrame。最终 DataFrame 保存为 `charac_posting.csv`，用于进一步分析。


## ***样本标签的概述（用于微调）***

在本研究中，我们旨在分析招聘数据并将其分类到标准职业分类（SOC）类别中。数据生成和清洗过程如下：

1. **读取数据**：  
   将原始招聘数据从 CSV 文件加载到 pandas DataFrame **`df`** 中。此步骤仅需要 `工作名称`（职位名称）列。

2. **过滤罕见职位名称**：  
   计算 DataFrame 中每个唯一职位名称的出现次数，过滤掉出现次数少于5次的职位名称，最终得到 883,695 个唯一职位名称和 50,340,840 条招聘记录。

3. **并行化职位名称分类**：  
   将包含已过滤职位名称的 DataFrame 分为 300 个较小的 DataFrame，并将每个子 DataFrame 保存为 CSV 文件。

4. **使用 ChatGPT 分类职位名称**：  
   定义一个函数 **`classify_job_title`**，该函数接受职位名称和 API 密钥，返回该职位名称的最可能的 SOC 代码。函数使用提供的 API 密钥向 ChatGPT API 发送请求，并从 API 响应中提取 SOC 代码。

5. **并行运行分类**：  
   使用 Python 的 `ThreadPoolExecutor` 创建一个包含 30 个工作线程的池以并发分类职位名称。从 CSV 文件中读取包含职位名称的子 DataFrame，将职位名称提交给线程池进行分类，将所得 SOC 代码附加到 DataFrame，并保存为新的 CSV 文件。

6. **合并已分类的标题**：  
   将所有已分类的标题的 CSV 文件附加到一个单一的 DataFrame **`df_title`** 中。仅保留 `工作名称` 和 `soc_code` 列，并删除任何缺失 SOC 代码的行。

7. **使用职位描述映射未映射的职位数据**：  
   将包含 SOC 代码的 DataFrame 与包含基于职位名称的职位描述的另一个 DataFrame 合并，以便映射剩余的未映射职位。

8. **加载 ONET SOC 职业名称**：  
   从 ONET 数据集中加载包含所有可能 SOC 职业名称的 DataFrame，以帮助消除错误映射。

9. **过滤罕见的 SOC 代码**：  
   过滤掉少于 100 个招聘数据的广义职业，以确保有足够的数据来训练良好的模型。此步骤最终得到 408 个广义职业。

10. **随机抽样职位数据**：  
    在每个广义职业中随机抽取 3,000 条职位数据并保存为 CSV 文件。将此数据输入到 ChatGPT 中，以使用职位描述将职位数据映射到 SOC 类别。

11. **基于职位描述验证标签**：  
    定义一个函数 **`classify_job_desp`**，该函数接受职位描述、职位名称和 API 密钥，并返回给定 SOC 代码是否基于职位描述的合理分类。该函数使用提供的 API 密钥向 ChatGPT API 发送请求，并返回一个是或否的答案。

12. **再次检查子样本数据集**：  
    使用 `ThreadPoolExecutor` 并行处理职位描述分类，并将真/假指示符附加到 DataFrame。将结果 DataFrame 保存为新 CSV 文件。

13. **生成最终数据集以进行模型微调**：  
    通过职位描述分类验证的最终数据集将用于模型微调。


## ***微调中文 BERT-wwm 的概述***

在此 Python 代码中，我们使用 `transformers` 库微调预训练的 BERT 模型，以根据标准职业分类（SOC）代码对招聘数据进行分类。以下是代码的分步骤详细说明。

1. **导入必要的库并加载数据**：  
   首先，我们导入 `pandas`、`torch` 和 `transformers` 等必需的库，并从 CSV 文件中读取数据。数据包括 `soc_code`、`true_ind`、`工作名称`（职位名称）和 `工作描述`（职位描述）等列。

2. **数据预处理**：  
   我们清理 `soc_code` 列，去除符号“-”并将其转换为整数。`true_ind` 列中将 'Yes' 替换为 `True`，`NaN` 替换为 `False`，指示标签是否更可靠。此可靠性指示符将在训练模型时用于分配不同的样本权重。

3. **数据分割**：  
   使用 `scikit-learn` 库的 `train_test_split` 函数，将数据按 60-20-20 比例分为训练、验证和测试集。

4. **创建新的 'soc_code1' 列**：  
   我们创建一个新的 `soc_code1` 列，用于将唯一的 SOC 代码映射到连续的整数标签。这简化了分类任务，并有助于模型学习数据中的模式。

5. **分词**：  
   使用 BERT 分词器对职位名称和职位描述进行分词，将文本转换为 BERT 模型可以理解的格式。

6. **创建自定义 PyTorch 数据集**：  
   创建自定义 PyTorch 数据集类 `JobPostingDataset`，用于存储分词后的文本、标签和权重。此类将用于生成数据加载器，以便于训练、验证和测试。

7. **准备数据加载器**：  
   使用自定义的 `JobPostingDataset` 类创建数据加载器，以便在训练、验证和测试过程中有效地分批加载数据。

8. **计算类别权重**：  
   为了处理类别不平衡问题，我们使用训练数据计算类别权重，并将其传递给 `CrossEntropyLoss` 准则，以确保模型在训练过程中更加关注少数类别。

9. **定义模型、优化器和学习率调度器**：  
   我们微调预训练的 BERT 模型用于我们的分类任务，输出标签数量设置为唯一的 `soc_code1` 值的数量。使用 `AdamW` 优化器和带有预热期的学习率调度器进行训练。

10. **使用早停法训练模型**：  
    使用包含基于验证损失的早停法的循环进行模型训练。如果验证损失在一定数量的连续时期内（由 `early_stopping_patience` 变量指定）没有改善，训练将停止以防止过拟合。

11. **评估模型**：  
    使用自定义评估函数在验证集上评估模型的性能，并计算验证损失。基于最低验证损失保存最佳模型。
