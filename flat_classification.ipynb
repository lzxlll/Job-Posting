{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import csv\n",
    "import os\n",
    "\n",
    "# Third-party library imports\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import (AdamW, BertForSequenceClassification, BertTokenizer, get_linear_schedule_with_warmup, get_scheduler, Trainer)\n",
    "\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"D:/Dropbox/Dropbox/vs_cloud/Job_posting_data/chinese-bert-wwm/\")\n",
    "\n",
    "df = pd.read_csv(\"G:/Data/job_posting/processed/finetune/est_sample.csv\", encoding = \"utf_8_sig\", on_bad_lines='skip', encoding_errors='ignore')\n",
    "# replace the symbol '-' to '.' in soc_code column, and convert soc_code to int\n",
    "df['soc_code'] = df['soc_code'].str.replace('-', '')\n",
    "# replace 'Yes' with True and NaN with False using the fillna() and astype() methods\n",
    "df['true_ind'] = df['true_ind'].fillna(False).astype(bool)\n",
    "# generate a new column 'soc_code1' with value to recode the 'soc_code' in ascending order. Create a dictionary to map unique soc_codes to sequential integer labels\n",
    "unique_soc_codes = sorted(df['soc_code'].unique())\n",
    "soc_code_dict  = {soc_code: i for i, soc_code in enumerate(unique_soc_codes)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### The dataset is split into train, validation, and test sets as follows:\n",
    "# The initial train_test_split call splits df into train_df_sample (60% of the data) and temp_df_sample (40% of the data).\n",
    "# The second train_test_split call further splits temp_df_sample into valid_df_sample (50% of temp_df_sample, or 20% of the original data) and test_df_sample (50% of temp_df_sample, or 20% of the original data).\n",
    "# So, the final ratio of the dataset split is 60% for training, 20% for validation, and 20% for testing. Create into train, validation and test set\n",
    "\n",
    "train_df_sample, temp_df_sample = train_test_split(df, test_size=0.4, random_state=42)\n",
    "valid_df_sample, test_df_sample = train_test_split(temp_df_sample, test_size=0.5, random_state=42)\n",
    "\n",
    "# export the train, validation and test set to csv\n",
    "train_df_sample.to_csv('F:/Data/job_posting/processed/finetune/train_df_sample.csv', index=False, encoding = \"utf_8_sig\", header=True)\n",
    "test_df_sample.to_csv('F:/Data/job_posting/processed/finetune/test_df_sample.csv', index=False, encoding = \"utf_8_sig\", header=True)\n",
    "valid_df_sample.to_csv('F:/Data/job_posting/processed/finetune/valid_df_sample.csv', index=False, encoding = \"utf_8_sig\", header=True)\n",
    "\n",
    "def preprocess_df(df, soc_code_dict):\n",
    "    # Drop the 'Unnamed: 0' column\n",
    "    df = df.drop(['Unnamed: 0'], axis=1)\n",
    "    # Generate a new column 'soc_code1' with mapped values from 'soc_code'\n",
    "    df['soc_code1'] = df['soc_code'].map(soc_code_dict)\n",
    "    return df\n",
    "\n",
    "# Applying the function to each DataFrame\n",
    "train_df_sample = preprocess_df(train_df_sample, soc_code_dict)\n",
    "test_df_sample = preprocess_df(test_df_sample, soc_code_dict)\n",
    "valid_df_sample = preprocess_df(valid_df_sample, soc_code_dict)\n",
    "\n",
    "# Function to extract titles, texts, and labels from a DataFrame\n",
    "def extract_data(df):\n",
    "    titles = df['工作名称'].astype(str).tolist()\n",
    "    texts = df['工作描述'].astype(str).tolist()\n",
    "    labels = df['soc_code1'].tolist()\n",
    "    return titles, texts, labels\n",
    "\n",
    "# Extracting data from each DataFrame\n",
    "train_titles, train_texts, train_labels = extract_data(train_df_sample)\n",
    "test_titles, test_texts, test_labels = extract_data(test_df_sample)\n",
    "valid_titles, valid_texts, valid_labels = extract_data(valid_df_sample)\n",
    "\n",
    "# If you have more \"credible\" or reliable labels in your dataset, you can leverage this information to improve the performance of your model by assigning different weights to the loss function during training. This way, the model will put more emphasis on learning from the credible samples.\n",
    "def assign_weight(true_ind):\n",
    "    if true_ind:\n",
    "        return 1.0\n",
    "    else:\n",
    "        return 0.5\n",
    "    \n",
    "# True for credible labels and False for less credible labels\n",
    "train_df_sample['weight'] = train_df_sample['true_ind'].apply(assign_weight)\n",
    "test_df_sample['weight'] = test_df_sample['true_ind'].apply(assign_weight)\n",
    "valid_df_sample['weight'] = valid_df_sample['true_ind'].apply(assign_weight)\n",
    "\n",
    "# Extract the weights\n",
    "train_weights = train_df_sample['weight'].tolist()\n",
    "valid_weights = valid_df_sample['weight'].tolist()\n",
    "test_weights = test_df_sample['weight'].tolist()\n",
    "\n",
    "# Create the JobPostingDataset class\n",
    "class JobPostingDataset(Dataset):\n",
    "    def __init__(self, titles, descriptions, labels, weights, tokenizer, max_length):\n",
    "        self.titles = titles\n",
    "        self.descriptions = descriptions\n",
    "        self.labels = labels\n",
    "        self.weights = weights\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.titles)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        title = self.titles[idx]\n",
    "        description = self.descriptions[idx]\n",
    "        label = self.labels[idx]\n",
    "        weight = self.weights[idx]\n",
    "\n",
    "        # Concatenate title and description, repeat the title to give it more importance\n",
    "        repeat_title = 2  # Adjust this value to control the importance of the title\n",
    "        text = (title + \" \") * repeat_title + description\n",
    "\n",
    "        # Tokenize the text\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            max_length=self.max_length,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "\n",
    "        # Return a tuple of the input tensors, label, and weight\n",
    "        return (\n",
    "            encoding[\"input_ids\"].squeeze(0),\n",
    "            encoding[\"attention_mask\"].squeeze(0),\n",
    "            torch.tensor(label, dtype=torch.long),\n",
    "            torch.tensor(weight, dtype=torch.float),\n",
    "        )\n",
    "\n",
    "# Create the datasets\n",
    "max_length = 512\n",
    "train_dataset = JobPostingDataset(train_titles, train_texts, train_labels, train_weights, tokenizer, max_length)\n",
    "valid_dataset = JobPostingDataset(valid_titles, valid_texts, valid_labels, valid_weights, tokenizer, max_length)\n",
    "test_dataset = JobPostingDataset(test_titles, test_texts, test_labels, test_weights, tokenizer, max_length)\n",
    "\n",
    "# Create the data loaders\n",
    "batch_size = 20\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=batch_size)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "### Define an evaluate() function to compute the validation loss\n",
    "def evaluate(model, valid_loader, device, loss_fn):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    num_batches = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in valid_loader:\n",
    "            inputs = batch[0].to(device)\n",
    "            masks = batch[1].to(device)\n",
    "            labels = batch[2].to(device)\n",
    "            weights = batch[3].to(device)\n",
    "\n",
    "            logits = model(inputs, attention_mask=masks).logits\n",
    "            batch_loss = loss_fn(logits, labels)\n",
    "            weighted_batch_loss = batch_loss * weights\n",
    "            loss = torch.mean(weighted_batch_loss)\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            num_batches += 1\n",
    "    return total_loss / num_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle class imbalance by adjusting class weights in the CrossEntropyLoss criterion. To achieve this, you need to compute class weights and pass them as an argument to the CrossEntropyLoss function.\n",
    "# The training loop to include early stopping based on the validation loss.\n",
    "# Define the model, the optimizer, and the learning rate scheduler\n",
    "\n",
    "num_labels = len(train_df_sample['soc_code1'].unique())\n",
    "model = BertForSequenceClassification.from_pretrained(\"G:/Other computers/我的计算机/cloud_share/Job_posting_data/chinese-bert-wwm/\", num_labels=num_labels)\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5, eps=1e-8)\n",
    "num_epochs = 300\n",
    "num_training_steps = num_epochs * len(train_loader)\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=num_training_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute class weights using the train_labels_np array\n",
    "unique_labels = train_df_sample['soc_code1'].unique()\n",
    "class_weights = compute_class_weight('balanced', classes=unique_labels, y=train_labels)\n",
    "class_weights = torch.tensor(class_weights, dtype=torch.float) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The patience parameter determines how many consecutive epochs the model can go without an improvement in validation loss before stopping the training. \n",
    "# In this case, the patience is set to 3, meaning that if the validation loss does not improve for 3 consecutive epochs, the training will be stopped.\n",
    "early_stopping_patience = 3\n",
    "\n",
    "# This line initializes a counter variable called num_epochs_without_improvement that keeps track of the number of consecutive epochs without an improvement in validation loss. \n",
    "# The counter is set to 0 at the beginning of the training process and is incremented by 1 whenever there is no improvement in the validation loss. If the validation loss improves in a particular epoch, the counter is reset to 0.\n",
    "num_epochs_without_improvement = 0\n",
    "\n",
    "# During the training loop, if num_epochs_without_improvement becomes equal to or greater than early_stopping_patience, the training will be stopped. \n",
    "# This way, the training process can be terminated early when the model starts overfitting, or when there is no significant improvement in the validation loss.\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "# Train the model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Initialize lists to store losses\n",
    "train_losses = []\n",
    "\n",
    "# Utilize multiple GPUs with DataParallel\n",
    "if torch.cuda.device_count() > 1:\n",
    "    model = torch.nn.DataParallel(model)\n",
    "\n",
    "# Pass the computed class_weights to the CrossEntropyLoss function:\n",
    "# Create a loss function that doesn't reduce the losses right away and pass class_weights\n",
    "# By incorporating class weights into the loss function, the model will pay more attention to the minority classes during training. \n",
    "loss_fn = torch.nn.CrossEntropyLoss(weight=class_weights.to(device), reduction='none')\n",
    "\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_train_loss = 0\n",
    "    num_batches = 0\n",
    "    for batch in train_loader:\n",
    "        inputs = batch[0].to(device)\n",
    "        masks = batch[1].to(device)\n",
    "        labels = batch[2].to(device)\n",
    "        weights = batch[3].to(device)  # Assuming the weights are the 4th element in the batch\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        logits = model(inputs, attention_mask=masks).logits\n",
    "\n",
    "        # Compute the loss for each sample\n",
    "        batch_loss = loss_fn(logits, labels)\n",
    "\n",
    "        # Multiply the loss by the corresponding weight\n",
    "        weighted_batch_loss = batch_loss * weights\n",
    "\n",
    "        # Average the weighted losses\n",
    "        loss = torch.mean(weighted_batch_loss)\n",
    "\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Add the current batch loss to the epoch_train_loss\n",
    "        epoch_train_loss += loss.item()\n",
    "        num_batches += 1\n",
    "\n",
    "    # Calculate average loss for the current epoch and append it to the train_losses list\n",
    "    epoch_train_loss /= num_batches\n",
    "    train_losses.append(epoch_train_loss)\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {epoch_train_loss:.4f}\")\n",
    "\n",
    "    # Evaluate the model on the validation set\n",
    "    valid_loss = evaluate(model, valid_loader, device, loss_fn)\n",
    "    print(f\"Validation Loss: {valid_loss:.4f}\")\n",
    "\n",
    "    # Save the best model based on the validation loss\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        if isinstance(model, torch.nn.DataParallel):\n",
    "            model.module.save_pretrained(\"F:/Data/job_posting/processed/finetune/best_model\")\n",
    "        else:\n",
    "            model.save_pretrained(\"F:/Data/job_posting/processed/finetune/best_model\")\n",
    "        num_epochs_without_improvement = 0\n",
    "    else:\n",
    "        num_epochs_without_improvement += 1\n",
    "\n",
    "    # Check the stopping condition and break the loop if needed\n",
    "    if num_epochs_without_improvement >= early_stopping_patience:\n",
    "        print(\"Early stopping due to no improvement in validation loss.\")\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
