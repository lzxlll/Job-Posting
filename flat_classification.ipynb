{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, AdamW, AutoModelForMaskedLM\n",
    "from transformers import BertForSequenceClassification\n",
    "import torch\n",
    "from transformers import AdamW\n",
    "from transformers import get_scheduler\n",
    "from transformers import Trainer\n",
    "from transformers import BertTokenizer\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv\n",
    "import os\n",
    "\n",
    "\n",
    "#tokenizer = AutoTokenizer.from_pretrained(\"bert-base-chinese\")\n",
    "tokenizer = BertTokenizer.from_pretrained(\"G:/cloud_share/Job_posting_data/chinese-bert-wwm/\")\n",
    "\n",
    "df = pd.read_csv(\"F:/Data/job_posting/processed/finetune/est_sample.csv\", encoding = \"utf_8_sig\", on_bad_lines='skip', encoding_errors='ignore')\n",
    "\n",
    "# replace the symbol '-' to '.' in soc_code column, and convert soc_code to int\n",
    "df['soc_code'] = df['soc_code'].str.replace('-', '')\n",
    "\n",
    "# replace 'Yes' with True and NaN with False using the fillna() and astype() methods\n",
    "df['true_ind'] = df['true_ind'].fillna(False).astype(bool)",
    "\n",
    "# Create a dictionary to map unique soc_codes to sequential integer labels\n",
    "unique_soc_codes = sorted(df['soc_code'].unique())\n",
    "soc_code_dict  = {soc_code: i for i, soc_code in enumerate(unique_soc_codes)}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The dataset is split into train, validation, and test sets as follows:\n",
    "\n",
    "- The initial train_test_split call splits df into train_df_sample (60% of the data) and temp_df_sample (40% of the data).\n",
    "- The second train_test_split call further splits temp_df_sample into valid_df_sample (50% of temp_df_sample, or 20% of the original data) and test_df_sample (50% of temp_df_sample, or 20% of the original data).\n",
    "- So, the final ratio of the dataset split is 60% for training, 20% for validation, and 20% for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create into train, validation and test set\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_df_sample, temp_df_sample = train_test_split(df, test_size=0.4, random_state=42)\n",
    "valid_df_sample, test_df_sample = train_test_split(temp_df_sample, test_size=0.5, random_state=42)\n",
    "\n",
    "# export the train, validation and test set to csv\n",
    "train_df_sample.to_csv('F:/Data/job_posting/processed/finetune/train_df_sample.csv', index=False, encoding = \"utf_8_sig\", header=True)\n",
    "test_df_sample.to_csv('F:/Data/job_posting/processed/finetune/test_df_sample.csv', index=False, encoding = \"utf_8_sig\", header=True)\n",
    "valid_df_sample.to_csv('F:/Data/job_posting/processed/finetune/valid_df_sample.csv', index=False, encoding = \"utf_8_sig\", header=True)\n",
    "\n",
    "\n",
    "# drop index column, 'true_ind' and 'sample' columns\n",
    "train_df_sample = train_df_sample.drop(['Unnamed: 0'], axis = 1)\n",
    "# Generate a new column 'soc_code1' with the mapped values from 'soc_code'\n",
    "train_df_sample['soc_code1'] = train_df_sample['soc_code'].map(soc_code_dict)\n",
    "\n",
    "\n",
    "# drop index column, 'true_ind' and 'sample' columns\n",
    "test_df_sample = test_df_sample.drop(['Unnamed: 0'], axis = 1)\n",
    "# Generate a new column 'soc_code1' with the mapped values from 'soc_code' for the test set\n",
    "test_df_sample['soc_code1'] = test_df_sample['soc_code'].map(soc_code_dict)\n",
    "\n",
    "# drop index column, 'true_ind' and 'sample' columns\n",
    "valid_df_sample = valid_df_sample.drop(['Unnamed: 0'], axis = 1)\n",
    "# Generate a new column 'soc_code1' with the mapped values from 'soc_code' for the validation set\n",
    "valid_df_sample['soc_code1'] = valid_df_sample['soc_code'].map(soc_code_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the text and convert it into input features\n",
    "train_titles = train_df_sample['工作名称'].astype(str).tolist()\n",
    "train_texts = train_df_sample['工作描述'].astype(str).tolist()\n",
    "train_labels = train_df_sample['soc_code1'].tolist()\n",
    "\n",
    "\n",
    "test_titles = test_df_sample['工作名称'].astype(str).tolist()\n",
    "test_texts = test_df_sample['工作描述'].astype(str).tolist()\n",
    "test_labels = test_df_sample['soc_code1'].tolist()\n",
    "\n",
    "\n",
    "valid_titles = valid_df_sample['工作名称'].astype(str).tolist()\n",
    "valid_texts = valid_df_sample['工作描述'].astype(str).tolist()\n",
    "valid_labels = valid_df_sample['soc_code1'].tolist()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- If you have more reliable labels in your dataset, you can leverage this information to improve the performance of your model by assigning different weights to the loss function during training. This way, the model will put more emphasis on learning from the credible samples. We assign a weight of 0.5 to the first checked sample and a weight of 1.0 to the second checked sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_weight(true_ind):\n",
    "    if true_ind:\n",
    "        return 1.0\n",
    "    else:\n",
    "        return 0.5\n",
    "    \n",
    "# Assuming you have a column called 'true_ind' in your dataset with boolean values\n",
    "# True for credible labels and False for less credible labels\n",
    "train_df_sample['weight'] = train_df_sample['true_ind'].apply(assign_weight)\n",
    "test_df_sample['weight'] = test_df_sample['true_ind'].apply(assign_weight)\n",
    "valid_df_sample['weight'] = valid_df_sample['true_ind'].apply(assign_weight)\n",
    "\n",
    "# Extract the weights\n",
    "train_weights = train_df_sample['weight'].tolist()\n",
    "valid_weights = valid_df_sample['weight'].tolist()\n",
    "test_weights = test_df_sample['weight'].tolist()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The text is based on both job posting titles and descriptions. To emphasize the importance of the job title, we repeat the title twice in the text. This way, the model will learn to give more importance to the title than the description."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "# Create the JobPostingDataset class\n",
    "class JobPostingDataset(Dataset):\n",
    "    def __init__(self, titles, descriptions, labels, weights, tokenizer, max_length):\n",
    "        self.titles = titles\n",
    "        self.descriptions = descriptions\n",
    "        self.labels = labels\n",
    "        self.weights = weights\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.titles)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        title = self.titles[idx]\n",
    "        description = self.descriptions[idx]\n",
    "        label = self.labels[idx]\n",
    "        weight = self.weights[idx]\n",
    "\n",
    "        # Concatenate title and description, repeat the title to give it more importance\n",
    "        repeat_title = 2  # Adjust this value to control the importance of the title\n",
    "        text = (title + \" \") * repeat_title + description\n",
    "\n",
    "        # Tokenize the text\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            max_length=self.max_length,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "\n",
    "        # Return a tuple of the input tensors, label, and weight\n",
    "        return (\n",
    "            encoding[\"input_ids\"].squeeze(0),\n",
    "            encoding[\"attention_mask\"].squeeze(0),\n",
    "            torch.tensor(label, dtype=torch.long),\n",
    "            torch.tensor(weight, dtype=torch.float),\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from transformers import BertForSequenceClassification, AdamW, get_linear_schedule_with_warmup\n",
    "\n",
    "# Create the datasets\n",
    "max_length = 512\n",
    "train_dataset = JobPostingDataset(train_titles, train_texts, train_labels, train_weights, tokenizer, max_length)\n",
    "valid_dataset = JobPostingDataset(valid_titles, valid_texts, valid_labels, valid_weights, tokenizer, max_length)\n",
    "test_dataset = JobPostingDataset(test_titles, test_texts, test_labels, test_weights, tokenizer, max_length)\n",
    "\n",
    "# Create the data loaders\n",
    "batch_size = 5\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=batch_size)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define an evaluate() function to compute the validation loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, valid_loader, device, loss_fn):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    num_batches = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in valid_loader:\n",
    "            inputs = batch[0].to(device)\n",
    "            masks = batch[1].to(device)\n",
    "            labels = batch[2].to(device)\n",
    "            weights = batch[3].to(device)\n",
    "\n",
    "            logits = model(inputs, attention_mask=masks).logits\n",
    "            batch_loss = loss_fn(logits, labels)\n",
    "            weighted_batch_loss = batch_loss * weights\n",
    "            loss = torch.mean(weighted_batch_loss)\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            num_batches += 1\n",
    "    return total_loss / num_batches\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Handle class imbalance by adjusting class weights in the CrossEntropyLoss criterion. To achieve this, you need to compute class weights and pass them as an argument to the CrossEntropyLoss function.\n",
    "- The training loop to include early stopping based on the validation loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model, the optimizer, and the learning rate scheduler\n",
    "num_labels = len(train_df_sample['soc_code1'].unique())\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained(\"G:/cloud_share/Job_posting_data/chinese-bert-wwm/\", num_labels=num_labels)\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5, eps=1e-8)\n",
    "num_epochs = 2\n",
    "num_training_steps = num_epochs * len(train_loader)\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=num_training_steps)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "# Compute class weights using the train_labels_np array\n",
    "unique_labels = train_df_sample['soc_code1'].unique()\n",
    "class_weights = compute_class_weight('balanced', classes=unique_labels, y=train_labels)\n",
    "class_weights = torch.tensor(class_weights, dtype=torch.float)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Here, we start to train the model and use the validation loss to determine the best model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The patience parameter determines how many consecutive epochs the model can go without an improvement in validation loss before stopping the training. \n",
    "# In this case, the patience is set to 3, meaning that if the validation loss does not improve for 3 consecutive epochs, the training will be stopped.\n",
    "early_stopping_patience = 3\n",
    "# This line initializes a counter variable called num_epochs_without_improvement that keeps track of the number of consecutive epochs without an improvement in validation loss. \n",
    "# The counter is set to 0 at the beginning of the training process and is incremented by 1 whenever there is no improvement in the validation loss. If the validation loss improves in a particular epoch, the counter is reset to 0.\n",
    "num_epochs_without_improvement = 0\n",
    "# During the training loop, if num_epochs_without_improvement becomes equal to or greater than early_stopping_patience, the training will be stopped. \n",
    "# This way, the training process can be terminated early when the model starts overfitting, or when there is no significant improvement in the validation loss.\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "# Train the model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Initialize lists to store losses\n",
    "train_losses = []\n",
    "\n",
    "# Utilize multiple GPUs with DataParallel\n",
    "if torch.cuda.device_count() > 1:\n",
    "    model = torch.nn.DataParallel(model)\n",
    "\n",
    "# Pass the computed class_weights to the CrossEntropyLoss function:\n",
    "# Create a loss function that doesn't reduce the losses right away and pass class_weights\n",
    "loss_fn = torch.nn.CrossEntropyLoss(weight=class_weights.to(device), reduction='none')\n",
    "# By incorporating class weights into the loss function, the model will pay more attention to the minority classes during training. \n",
    "\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_train_loss = 0\n",
    "    num_batches = 0\n",
    "    for batch in train_loader:\n",
    "        inputs = batch[0].to(device)\n",
    "        masks = batch[1].to(device)\n",
    "        labels = batch[2].to(device)\n",
    "        weights = batch[3].to(device)  # Assuming the weights are the 4th element in the batch\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        logits = model(inputs, attention_mask=masks).logits\n",
    "\n",
    "        # Compute the loss for each sample\n",
    "        batch_loss = loss_fn(logits, labels)\n",
    "\n",
    "        # Multiply the loss by the corresponding weight\n",
    "        weighted_batch_loss = batch_loss * weights\n",
    "\n",
    "        # Average the weighted losses\n",
    "        loss = torch.mean(weighted_batch_loss)\n",
    "\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Add the current batch loss to the epoch_train_loss\n",
    "        epoch_train_loss += loss.item()\n",
    "        num_batches += 1\n",
    "\n",
    "    # Calculate average loss for the current epoch and append it to the train_losses list\n",
    "    epoch_train_loss /= num_batches\n",
    "    train_losses.append(epoch_train_loss)\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {epoch_train_loss:.4f}\")\n",
    "\n",
    "    # Evaluate the model on the validation set\n",
    "    valid_loss = evaluate(model, valid_loader, device, loss_fn)\n",
    "    print(f\"Validation Loss: {valid_loss:.4f}\")\n",
    "\n",
    "    # Save the best model based on the validation loss\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        if isinstance(model, torch.nn.DataParallel):\n",
    "            model.module.save_pretrained(\"F:/Data/job_posting/processed/finetune/best_model\")\n",
    "        else:\n",
    "            model.save_pretrained(\"F:/Data/job_posting/processed/finetune/best_model\")\n",
    "        num_epochs_without_improvement = 0\n",
    "    else:\n",
    "        num_epochs_without_improvement += 1\n",
    "\n",
    "    # Check the stopping condition and break the loop if needed\n",
    "    if num_epochs_without_improvement >= early_stopping_patience:\n",
    "        print(\"Early stopping due to no improvement in validation loss.\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Store the training loss, use matplotlib to visualize the loss trends."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(np.arange(1, num_epochs + 1), train_losses, label='Training Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss per Epoch')\n",
    "plt.legend()\n",
    "\n",
    "# Save the figure\n",
    "plt.savefig('F:/Data/job_posting/processed/finetune/training_loss_plot.pdf', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Measure the model performance and visualize it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the performance of the model on the test set\n",
    "model.eval()\n",
    "predictions = []\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        inputs = batch[0].to(device)\n",
    "        masks = batch[1].to(device)\n",
    "\n",
    "        # Resize the attention mask tensor to match the size of the inputs\n",
    "        # masks.resize_(inputs.shape[0], inputs.shape[1])\n",
    "    \n",
    "        outputs = model(inputs, attention_mask=masks)\n",
    "        logits = outputs.logits\n",
    "        batch_predictions = torch.argmax(logits, axis=1).cpu().numpy()\n",
    "        predictions.extend(batch_predictions)\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy = accuracy_score(test_labels, predictions)\n",
    "print(f\"Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We first generate some features that can be used later to visualize the model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, we need to modify the true_labels and predictions by extracting the major SOC groups from the original SOC codes.\n",
    "# Create a list of SOC codes ordered by their corresponding sequential labels\n",
    "ordered_soc_codes = [soc_code for soc_code, _ in sorted(soc_code_dict.items(), key=lambda item: item[1])]\n",
    "\n",
    "# Convert test_labels and predictions back to SOC codes\n",
    "test_labels_soc = np.array([ordered_soc_codes[label] for label in test_labels])\n",
    "predictions_soc = np.array([ordered_soc_codes[label] for label in predictions])\n",
    "\n",
    "# Extract major SOC groups\n",
    "test_major_groups = np.array([soc_code[:2] for soc_code in test_labels_soc])\n",
    "pred_major_groups = np.array([soc_code[:2] for soc_code in predictions_soc])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Visualize the accuracy\n",
    "  * To make it easier to read, the accuracy is aggregated at the major SOC groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate accuracy for each major SOC group\n",
    "major_group_accuracies = {}\n",
    "unique_major_groups = np.unique(test_major_groups)\n",
    "for major_group in unique_major_groups:\n",
    "    major_group_indices = np.where(test_major_groups == major_group)\n",
    "    major_group_accuracy = accuracy_score(test_major_groups[major_group_indices], pred_major_groups[major_group_indices])\n",
    "    major_group_accuracies[major_group] = major_group_accuracy\n",
    "    print(f\"Major Group {major_group}: Accuracy: {major_group_accuracy:.4f}\")\n",
    "\n",
    "major_group_accuracies_df = pd.DataFrame(list(major_group_accuracies.items()), columns=['Major Group', 'Accuracy'])\n",
    "major_group_accuracies_df.to_csv('F:/Data/job_posting/processed/finetune/major_group_accuracies.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modify the bar plot code to plot the accuracy for each major SOC group\n",
    "labels = major_group_accuracies_df['Major Group'].tolist()\n",
    "accuracies = major_group_accuracies_df['Accuracy'].tolist()\n",
    "\n",
    "x = np.arange(len(labels))  # The label locations\n",
    "width = 0.35  # The width of the bars\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15, 6))\n",
    "rects = ax.bar(x, accuracies, width)\n",
    "\n",
    "# Add some text for labels, title, and custom x-axis tick labels, etc.\n",
    "ax.set_ylabel('Accuracy')\n",
    "ax.set_title('Model Accuracy on Test Set for Each Major SOC Group')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(labels)\n",
    "ax.set_ylim(0, 1)  # Set y-axis limit between 0 and 1\n",
    "\n",
    "# Function to add labels above the bars\n",
    "def autolabel(rects):\n",
    "    for rect in rects:\n",
    "        height = rect.get_height()\n",
    "        ax.annotate('{:.4f}'.format(height),\n",
    "                    xy=(rect.get_x() + rect.get_width() / 2, height),\n",
    "                    xytext=(0, 3),  # 3 points vertical offset\n",
    "                    textcoords=\"offset points\",\n",
    "                    ha='center', va='bottom')\n",
    "\n",
    "#autolabel(rects)\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.subplots_adjust(top=0.9)  # Adjust the top margin to avoid overlap\n",
    "\n",
    "# Save the figure\n",
    "plt.savefig('F:/Data/job_posting/processed/finetune/major_group_accuracy_plot.pdf', bbox_inches='tight')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Confusion Matrix: A confusion matrix is a table that shows the predicted class against the true class. It helps to identify which classes the model is correctly predicting and where it's making mistakes.\n",
    "  * To make it easier to read, the accuracy is aggregated at the major SOC groups.\n",
    "  * A raw plot would have too many SOC labels, making it hard to interpret."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import BertForSequenceClassification\n",
    "\n",
    "# Load the best model\n",
    "best_model = BertForSequenceClassification.from_pretrained(\"F:/Data/job_posting/processed/finetune/best_model\")\n",
    "best_model.to(device)\n",
    "best_model.eval()\n",
    "\n",
    "# First, we need to modify the true_labels and predictions by extracting the major SOC groups from the original SOC codes.\n",
    "# Create a list of unique major SOC groups\n",
    "unique_major_groups = sorted(list(set(test_major_groups).union(set(pred_major_groups))))\n",
    "\n",
    "# Generate predictions on test set\n",
    "predictions = []\n",
    "true_labels = []\n",
    "\n",
    "for batch in test_loader:\n",
    "    inputs = batch[0].to(device)\n",
    "    masks = batch[1].to(device)\n",
    "    labels = batch[2].to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        logits = best_model(inputs, attention_mask=masks).logits\n",
    "\n",
    "    preds = torch.argmax(logits, dim=-1).cpu().numpy()\n",
    "    predictions.extend(preds)\n",
    "    true_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "# Compute confusion matrix\n",
    "# cm = confusion_matrix(true_labels, predictions)\n",
    "# Compute the aggregated confusion matrix\n",
    "aggregated_cm = confusion_matrix(test_major_groups, pred_major_groups)\n",
    "\n",
    "# Plot confusion matrix using seaborn heatmap with actual SOC codes as xticklabels and yticklabels\n",
    "plt.figure(figsize=(10, 10))\n",
    "sns.heatmap(aggregated_cm, annot=False, fmt='d', cmap='Blues', xticklabels=unique_major_groups, yticklabels=unique_major_groups)\n",
    "plt.xlabel('Predicted Major SOC Group')\n",
    "plt.ylabel('True Major SOC Group')\n",
    "plt.title('Aggregated Confusion Matrix')\n",
    "\n",
    "# Save the figure\n",
    "plt.savefig('F:/Data/job_posting/processed/finetune/confusion_matrix.pdf', bbox_inches='tight')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Precision-Recall Curve: The precision-recall curve shows the trade-off between precision and recall for different decision thresholds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.preprocessing import label_binarize\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "n_classes = len(np.unique(test_labels))\n",
    "\n",
    "# Binarize the labels\n",
    "y_true_bin = label_binarize(test_labels, classes=np.arange(n_classes))\n",
    "y_pred_bin = label_binarize(predictions, classes=np.arange(n_classes))\n",
    "\n",
    "# Create a reverse mapping dictionary to convert the sequential labels back to SOC labels\n",
    "reverse_soc_code_dict = {v: k for k, v in soc_code_dict.items()}\n",
    "\n",
    "# Plot the precision-recall curve for each class\n",
    "for i in range(n_classes):\n",
    "    precision, recall, _ = precision_recall_curve(y_true_bin[:, i], y_pred_bin[:, i])\n",
    "\n",
    "    # Get the actual SOC label for the current class\n",
    "    actual_soc_label = reverse_soc_code_dict[i]\n",
    "\n",
    "    plt.plot(recall, precision, label=f\"{actual_soc_label}\")\n",
    "\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Precision-Recall Curve')\n",
    "#plt.legend(loc='upper center', fontsize='xx-small', bbox_to_anchor=(0.5, -0.15), title='SOC classifcation')\n",
    "\n",
    "# Save the figure\n",
    "plt.savefig('F:/Data/job_posting/processed/finetune/precision_recall.pdf', bbox_inches='tight')\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Classification Report: It shows the precision, recall, F1-score, and support for each class.\n",
    "  - first, we generate a table for all the SOC groups.\n",
    "  - second, we visualize the statistics for the major SOC groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "import pandas as pd\n",
    "\n",
    "# Evaluate the model and store predictions\n",
    "model.eval()\n",
    "\n",
    "predictions = []\n",
    "true_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        inputs = batch[0].to(device)\n",
    "        masks = batch[1].to(device)\n",
    "        labels = batch[2].to(device)\n",
    "\n",
    "        logits = model(inputs, attention_mask=masks).logits\n",
    "        preds = torch.argmax(logits, dim=1).cpu().numpy()\n",
    "        labels = labels.cpu().numpy()\n",
    "\n",
    "        predictions.extend(preds)\n",
    "        true_labels.extend(labels)\n",
    "\n",
    "# Convert predictions and true_labels into NumPy arrays\n",
    "predictions = np.array(predictions)\n",
    "true_labels = np.array(true_labels)\n",
    "\n",
    "# Map the sequential labels back to the original SOC codes\n",
    "soc_predictions = [reverse_soc_code_dict[p] for p in predictions]\n",
    "soc_true_labels = [reverse_soc_code_dict[l] for l in true_labels]\n",
    "\n",
    "# Compute classification report\n",
    "unique_labels = sorted(list(set(soc_true_labels).union(set(soc_predictions))))\n",
    "report = classification_report(soc_true_labels, soc_predictions, labels=unique_labels, output_dict=True)\n",
    "\n",
    "# Convert report to a pandas DataFrame\n",
    "report_df = pd.DataFrame(report).transpose()\n",
    "\n",
    "# Print the report\n",
    "print(report_df)\n",
    "report_df.to_csv('F:/Data/job_posting/processed/finetune/report_df.csv', index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize it\n",
    "# This code defines a function visualize_classification_report that takes the classification report DataFrame as input and displays a heatmap. \n",
    "# The heatmap shows precision, recall, and F1-score for each class. \n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define a function to visualize the classification report\n",
    "def visualize_classification_report(report_df, heatmap_filepath='heatmap.png'):\n",
    "    # Remove the 'support' column and the last row ('accuracy', 'macro avg', 'weighted avg')\n",
    "    heatmap_data = report_df.iloc[:-3, :-1]\n",
    "\n",
    "    # Sort the major SOC groups\n",
    "    heatmap_data = heatmap_data.sort_index()\n",
    "\n",
    "    # Plot the heatmap\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.heatmap(heatmap_data, annot=True, cmap='coolwarm', fmt='.2f', cbar=False, linewidths=0.5)\n",
    "    plt.xlabel('Metrics')\n",
    "    plt.ylabel('Major SOC Groups')\n",
    "    plt.title('Classification Report Heatmap')\n",
    "\n",
    "    # Save the heatmap as an image file\n",
    "    plt.savefig(heatmap_filepath, bbox_inches='tight')\n",
    "\n",
    "    # Show the heatmap\n",
    "    plt.show()\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Calculate the classification report\n",
    "report = classification_report(test_major_groups, pred_major_groups, output_dict=True)\n",
    "\n",
    "# Convert the report to a DataFrame\n",
    "report_df = pd.DataFrame(report).transpose()\n",
    "\n",
    "report_df['major_group'] = report_df.index\n",
    "\n",
    "# Group the DataFrame by the major SOC groups and calculate the mean of the metrics for each group\n",
    "grouped_report_df = report_df.groupby('major_group').mean()\n",
    "\n",
    "# Call the function to visualize the classification report\n",
    "visualize_classification_report(grouped_report_df, 'F:/Data/job_posting/processed/finetune/report_df.pdf')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
