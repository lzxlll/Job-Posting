{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding:utf-8 -*-\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Main():\n",
    "    source_dir = '/Dropbox/job_posting_data/recruit_ns_210806.dat'\n",
    "    target_dir = '/Desktop'\n",
    " \n",
    "    # 计数器\n",
    "    flag = 0\n",
    " \n",
    "    # 文件名\n",
    "    name = 1\n",
    " \n",
    "    # 存放数据\n",
    "    dataList = []\n",
    " \n",
    "    print(\"Start\")\n",
    "    print(datetime.now().strftime('%Y-%m-%d %H:%M:%S'))\n",
    " \n",
    "    with open(source_dir,'r', encoding=\"utf8\") as f_source:\n",
    "        for line in f_source:\n",
    "          flag+=1\n",
    "          dataList.append(line)\n",
    "          if flag == 1000000:\n",
    "              with open(target_dir+\"job_posting_\"+str(name)+\".dat\",'w+', encoding=\"utf8\", newline='\\r\\n') as f_target:\n",
    "                  for data in dataList:\n",
    "                      f_target.write(data)\n",
    "              name+=1\n",
    "              flag = 0\n",
    "              dataList = []\n",
    "                \n",
    "    # 处理最后一批行数少于100万行的\n",
    "    with open(target_dir+\"job_posting_\"+str(name)+\".dat\",'w+', encoding=\"utf8\", newline='\\r\\n') as f_target:\n",
    "        for data in dataList:\n",
    "            f_target.write(data)\n",
    " \n",
    "    print(\"Finish\")\n",
    "    print(datetime.now().strftime('%Y-%m-%d %H:%M:%S'))\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start\n",
      "2021-10-25 10:57:41\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    Main()"
   ]
  },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2c8efa0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the code to screen job posting data\n",
    "# -*- coding: utf-8 -*-\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import gc, json, csv, re, os, glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c26b6b34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_dat_data(curFile):\n",
    "    colNames = ['招聘主键ID','公司ID','公司名称','城市名称','公司所在区域','工作薪酬','教育要求','工作经历',\n",
    "                '工作描述','职位名称','工作名称','招聘数量','发布日期','行业名称','数据来源']\n",
    "    resCSV = pd.read_csv(curFile, header=None, index_col=None, names=colNames,encoding='utf-8',quoting=csv.QUOTE_NONE, sep=\"@!\", error_bad_lines=False, engine='python')\n",
    "    return resCSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d86b4fff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The first purge of data:\n",
    "# 1. We use 工作名称 as the title to feed the ChatGPT. \n",
    "# 2. We drop all the publish data is missing.\n",
    "# 3. When 工作名称 is missing, we replace it with 职位名称.\n",
    "# 4. We drop title of ``part-time\"。\n",
    "# 5. We drop same '公司ID', '工作名称', '城市名称' within a month, because we treat this case as duplicates (same job posting been published multiple times)\n",
    "# 6. Only keep the data from 10 major job posting websites.\n",
    "# define the file location\n",
    "dataNameTmp = \"E:/Data/job_posting/Raw_data/job_posting_%s.dat\"\n",
    "\n",
    "naSum = 0\n",
    "dupSum = 0\n",
    "\n",
    "for i in range(1,142): \n",
    "    curFile = dataNameTmp%i\n",
    "        \n",
    "    print(curFile,\" is Grouping Computing...\")\n",
    "    datDf = read_dat_data(curFile)\n",
    "    #datDf.loc[datDf['职位名称'].str.contains('%',na=False),'职位名称'] = np.NaN\n",
    "    \n",
    "    datDf = datDf.replace(r'\\N',np.NaN).dropna(subset=['发布日期'])\n",
    "    datDf['工作名称'] = datDf[['工作名称']].replace(r'\\N',np.NaN)\n",
    "    datDf.loc[datDf['工作名称'].isna(),'工作名称'] = datDf.loc[datDf['工作名称'].isna(),'职位名称']\n",
    "    datDf = datDf[datDf['工作名称'] != \"兼职\"]\n",
    "     # subset the data to only include the '来源' == '智联招聘', '前程无忧', '拉勾网', 'BOSS直聘', '58同城', '猎聘网', '看准网', 百姓网', '拉勾网', '猎聘', '赶集网'， 'BOSS'\n",
    "    datDf = datDf[datDf['数据来源'].isin(['智联招聘', '前程无忧', '拉勾网', 'BOSS直聘', '58同城', '猎聘网', '看准网', '百姓网', '拉勾网', '猎聘', '赶集网', 'BOSS'])]\n",
    "\n",
    "    curNa = datDf.shape[0]\n",
    "    naSum = naSum + curNa\n",
    "    \n",
    "    datDf['date'] = datDf['发布日期'].apply(lambda x: x[0:7])\n",
    "    datDf = datDf.drop_duplicates(subset=['公司ID', '工作名称', '城市名称', 'date'], keep='first').reset_index(drop=True)\n",
    "    \n",
    "    curDup = datDf.shape[0]\n",
    "    dupSum = dupSum + curDup\n",
    "    \n",
    "    print(curFile,\"删除空值剩余: %s\"%curNa, \"去重复值剩余：%s\"%curDup)\n",
    "    datDf.to_csv('E:/Data/job_posting/mapped_job_posting/Update file/job_res_{}.csv'.format(i), sep='?', encoding = 'utf_8_sig', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6313119",
   "metadata": {},
   "outputs": [],
   "source": [
    "# From the master data, we separate ``job description\" so that rest of the data are manageable. \n",
    "\n",
    "directory = 'E:/Data/job_posting/mapped_job_posting/Update file/'\n",
    "\n",
    "# iterate over files in that directory\n",
    "for filename in os.listdir(directory):\n",
    "    # checking if it is a file\n",
    "    if filename.startswith(\"job_res_\"): # for files start with a prefix #\n",
    "        f = os.path.join(directory, filename)\n",
    "        df = pd.read_csv(f, encoding = \"utf_8_sig\", on_bad_lines='skip', delimiter= \"?\", header=None, encoding_errors='ignore')\n",
    "        df.rename(columns={0: '招聘主键ID', 1: '公司ID', 2: '公司名称', 3: '城市名称', 4: '公司所在区域', 5: '工作薪酬', 6: '教育要求', \n",
    "                   7: '工作经历', 8: '工作描述', 9: '职位名称', 10: '工作名称', 11: '招聘数量', 12: '发布日期', 13: '行业名称', \n",
    "                   14: '数据来源'}, inplace=True)\n",
    "        df_charac = df[['招聘主键ID', '公司ID', '公司名称', '城市名称', '公司所在区域', '工作薪酬', '教育要求', '工作经历', '职位名称', \n",
    "         '工作名称', '招聘数量', '发布日期', '行业名称', '数据来源']]\n",
    "        \n",
    "        # export the data to csv, use the header and set the encoding to utf-8\n",
    "        df_charac.to_csv('E:/Data/job_posting/processed/charac/{}'.format(filename), encoding = \"utf_8_sig\", header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61723c0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Append all the character data together, then generate a list of the job titles that used to feed to the ChatGPT\n",
    "\n",
    "os.chdir(\"E:/Data/job_posting/processed/charac\")\n",
    "extension = 'csv'\n",
    "all_filenames = [i for i in glob.glob('*.{}'.format(extension))]\n",
    "#combine all files in the list\n",
    "combined_csv = pd.concat([pd.read_csv(f, encoding = \"utf_8_sig\", on_bad_lines='skip', usecols = ['工作名称']) for f in all_filenames], ignore_index=True)\n",
    "# This is the complete list of job posting titles \n",
    "combined_csv.to_csv('E:/Data/job_posting/processed/estimation/charac_posting.csv', index=False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72e3464c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save all the ``job description\" data\n",
    "\n",
    "directory = 'E:/Data/job_posting/mapped_job_posting/Update file/'\n",
    "\n",
    "# iterate over files in that directory\n",
    "for filename in os.listdir(directory):\n",
    "    # checking if it is a file\n",
    "    if filename.startswith(\"job_res_\"): # for files start with a prefix #\n",
    "        f = os.path.join(directory, filename)\n",
    "        df = pd.read_csv(f, encoding = \"utf_8_sig\", on_bad_lines='skip', delimiter= \"?\", header=None, encoding_errors='ignore')\n",
    "        df.rename(columns={0: '招聘主键ID', 1: '公司ID', 2: '公司名称', 3: '城市名称', 4: '公司所在区域', 5: '工作薪酬', 6: '教育要求', \n",
    "                   7: '工作经历', 8: '工作描述', 9: '职位名称', 10: '工作名称', 11: '招聘数量', 12: '发布日期', 13: '行业名称', \n",
    "                   14: '数据来源'}, inplace=True)\n",
    "        df_desp = df[['招聘主键ID', '公司ID', '工作描述']]\n",
    "        df_desp.to_csv('E:/Data/job_posting/processed/description/{}'.format(filename))\n",
    "        \n",
    "        del df_desp\n",
    "        del df\n",
    "        gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "004c7674",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Below are the test code\n",
    "combined_csv.to_csv('E:/Data/job_posting/processed/estimation/charac_posting.csv', index=False, header=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5b71bd63",
   "metadata": {},
   "source": [
    "### Note: \n",
    "\n",
    "- The cell below is used to determine the website source, it has been determined and does not need to be run again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eea5509",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine the data source, we limit to Top 10 job posting websites to avoid fuzzywuzzy in the data source.\n",
    "dataNameTmp = \"E:/Data/job_posting/Raw_data/job_posting_%s.dat\"\n",
    "\n",
    "df_list = []\n",
    "\n",
    "for i in range(1,3): \n",
    "    curFile = dataNameTmp%i\n",
    "        \n",
    "    print(curFile,\" is Grouping Computing...\")\n",
    "    datDf = read_dat_data(curFile)\n",
    "\n",
    "\n",
    "    # count number of occurrences of each value in column '数据来源', generate a new column to record the count\n",
    "    datDf['count'] = datDf.groupby('数据来源')['数据来源'].transform('count')\n",
    "\n",
    "    # drop duplicates based on column '数据来源', only keep the first occurrence\n",
    "    datDf = datDf.drop_duplicates(subset=['数据来源'], keep='first').reset_index(drop=True)\n",
    "    datDf = datDf[['数据来源', 'count']]\n",
    "\n",
    "    df_list.append(datDf)\n",
    "        \n",
    "    #atDf.to_csv('F:/Data/job_posting/processed/temp/job_res_{}.csv'.format(i), sep='?', encoding = 'utf_8_sig', index=False)\n",
    "final_df = pd.concat(df_list)\n",
    "\n",
    "# group by '数据来源' and sum the count\n",
    "final_df = final_df.groupby('数据来源').sum().reset_index()\n",
    "\n",
    "# sort the dataframe based on column 'count'\n",
    "final_df = final_df.sort_values(by=['count'], ascending=False)\n",
    "final_df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e007b1bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
