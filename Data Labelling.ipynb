{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\DELL\\anaconda3\\lib\\site-packages\\scipy\\__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.2\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "# Standard library imports\n",
    "import gc\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "import urllib3\n",
    "import csv\n",
    "\n",
    "# Third-party library imports\n",
    "import numpy as np\n",
    "import openai\n",
    "import pandas as pd\n",
    "import requests\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from datetime import datetime\n",
    "from glob import glob\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "# Set up the OpenAI organization and API key for subsequent API calls.\n",
    "openai.organization = \"org-**********************\"\n",
    "openai.api_key = \"sk-**********************\"\n",
    "api_key = \"sk-**********************\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Load the job posting data, specifically extracting the '工作名称' (job titles) column. Determine the shape of the loaded DataFrame.\n",
    "df = pd.read_csv('PATH/charac_posting.csv', encoding = \"utf_8_sig\", on_bad_lines='skip', usecols = ['工作名称'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Then, we clean this dataframe by filtering out the job posting titles with duplicates less than 5 times.\n",
    "# Count the occurrences of each unique job title in the DataFrame to identify duplicates.\n",
    "df_counted = df['工作名称'].value_counts()\n",
    "# Filter out job titles that occur fewer than 5 times. The result is a filtered DataFrame with more frequently occurring titles.\n",
    "df_filtered = df_counted[df_counted>5]\n",
    "# Calculate the total number of job postings in the filtered DataFrame.\n",
    "df_filtered.sum()\n",
    "# Convert the series 'df_filtered' to a DataFrame and reset the index. Rename columns to '工作名称' for job titles and 'count' for their occurrences.\n",
    "df_filtered = df_filtered.to_frame().reset_index()\n",
    "df_filtered.columns = ['工作名称', 'count']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Parallelize the job title classification using Python's ThreadPoolExecutor, we feed the job titles to ChatGPT to map it to a SOC category.\n",
    "# Split the DataFrame into 300 smaller sub-DataFrames for more manageable processing.\n",
    "sub_dfs = np.array_split(df_filtered, 300)\n",
    "\n",
    "# Export each sub-DataFrame to a separate CSV file for further processing.\n",
    "for i, sub_df in enumerate(sub_dfs):\n",
    "    sub_df.to_csv(f'PATH/title_{i+1}.csv', index=True, encoding = \"utf_8_sig\")\n",
    "def classify_job_title(job_title, api_key):\n",
    "    # Define a function to classify a given job title using the OpenAI API. The function sends a request to the OpenAI API and returns the SOC (Standard Occupational Classification) code for the given job title.\n",
    "    url = 'https://api.openai.com/v1/chat/completions'\n",
    "    headers = {'Content-Type': 'application/json',\n",
    "               'Authorization': f'Bearer {api_key}'}\n",
    "    data = {'model': 'gpt-3.5-turbo-0301',\n",
    "            'messages':[\n",
    "                {\n",
    "                'role': 'user', \n",
    "                'content': f'The most likely Standard Occupational Classification title and code the occupation fall into: \"{job_title}\", only show the SOC code.'\n",
    "                }\n",
    "                       ]\n",
    "            }\n",
    "    try:\n",
    "        response = requests.post(url, headers=headers, json=data, verify=False)\n",
    "        response_data = json.loads(response.text)\n",
    "        if response.status_code != 200:\n",
    "            raise Exception(response_data['error']['message'])\n",
    "        return response_data['choices'][0]['message']['content']\n",
    "    except Exception as e:\n",
    "        print(f'Error occurred: {e}')\n",
    "        return 'N/A'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parallelize the job title classification using Python's ThreadPoolExecutor\n",
    "\n",
    "# Disable SSL warnings\n",
    "urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\n",
    "os.environ[\"http_proxy\"] = \"http://127.0.0.1:10809\"\n",
    "os.environ[\"https_proxy\"] = \"http://127.0.0.1:10809\"\n",
    "\n",
    "# Set up a ThreadPoolExecutor with 30 worker threads for parallel processing.\n",
    "with ThreadPoolExecutor(max_workers=30) as executor:\n",
    "    # Loop through each file in the range and process job titles.\n",
    "    for i in range(1, 301):\n",
    "        # read the file into a dataframe\n",
    "        filename = f'PATH/title_{i}.csv'\n",
    "        df = pd.read_csv(filename, encoding=\"utf_8_sig\", on_bad_lines='skip')\n",
    "\n",
    "        # Submit each job title in the file to the classify_job_title function using the thread pool.\n",
    "        job_titles = df['工作名称'].tolist()\n",
    "        futures = [executor.submit(classify_job_title, job_title, api_key) for job_title in job_titles]\n",
    "\n",
    "        # wait for all threads to complete and get the results\n",
    "        # create an empty list to store soc codes\n",
    "        soc_codes = []\n",
    "        for future in futures:\n",
    "            soc_code = future.result()\n",
    "            soc_codes.append(soc_code)\n",
    "\n",
    "        # Append the SOC codes obtained from the classification to the DataFrame.\n",
    "        df['soc_code'] = soc_codes\n",
    "        # Extract only the SOC code part from the results.\n",
    "        df['soc_code'] = df['soc_code'].str.extract(r'(\\d{2}-\\d{4})')\n",
    "        # Save the updated DataFrame to a new CSV file.\n",
    "        df.to_csv(f'PATH/title_{i}.csv', encoding=\"utf_8_sig\") \n",
    "        del df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Combine all processed CSV files into a single DataFrame.\n",
    "path = r'PATH/title_mapped' # use your path\n",
    "all_files = glob(os.path.join(path, \"*.csv\"))\n",
    "df_from_each_file = (pd.read_csv(f, encoding=\"utf_8_sig\") for f in all_files)\n",
    "df_title = pd.concat(df_from_each_file, ignore_index=True)\n",
    "# Keep only the '工作名称' and 'soc_code' columns and drop rows with missing SOC codes.\n",
    "df_title = df_title[['工作名称', 'soc_code']].dropna(subset=['soc_code'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### The next step is to map the rest unmapped job postings to the SOCs using the job descriptions.\n",
    "directory = 'PATH/'\n",
    "# create an empty DataFrame to store merged data\n",
    "df_titleLabel = pd.DataFrame()\n",
    "\n",
    "# Iterate over files in the directory and merge them with the 'df_title' DataFrame.\n",
    "for filename in os.listdir(directory):\n",
    "    if filename.startswith(\"job_res_\"):\n",
    "        f = os.path.join(directory, filename)\n",
    "        df = pd.read_csv(f, encoding=\"utf_8_sig\", on_bad_lines='skip', delimiter=\"?\")\n",
    "        df.rename(columns={'招聘ID': '招聘主键ID'}, inplace=True)\n",
    "        df = df[['招聘主键ID', '工作描述', '工作名称']]\n",
    "        df_titleData = pd.merge(df, df_title, on='工作名称', how='inner')\n",
    "        df_titleLabel = df_titleLabel.append(df_titleData)\n",
    "\n",
    "    # Convert the 'soc_code' column to a string type for consistency.\n",
    "    df_titleLabel['soc_code'] = df_titleLabel['soc_code'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### To do this, we first load the dataframe from ONET which contains all the possible SOC job titles. This step helps to remove incorrect mapping.\n",
    "df_soc = pd.read_csv('PATH/2019_to_SOC_Crosswalk.csv')\n",
    "df_soc = df_soc[['2018 SOC Code']].drop_duplicates()\n",
    "df_soc['2018 SOC Code'] = df_soc['2018 SOC Code'].str[:-1] + '0'\n",
    "df_soc.rename(columns={'2018 SOC Code': 'soc_code'}, inplace=True)\n",
    "df_soc = df_soc.drop_duplicates(subset=['soc_code'], keep='first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### We set the finest level to 6 digits, which already covers 459 broad occupations. The concern of going into more granular level is that the number of job postings will be too small to train a good model.\n",
    "# However, not all the broad occupations show up equally in the dataset. We filter out the broad occupations with less than 100 job postings. In this way, it has enough observation to train a good model.\n",
    "# We end up with 408 broad occupations. We randonmly sample 3000 job postings within each broad occupations, and save them to csv files. We feed this data to ChatGPT to map the job postings to the SOC categories by using job descriptions.\n",
    "\n",
    "# replace the last digit of 'soc_code' with '0'\n",
    "df_titleLabel['soc_code'] = df_titleLabel['soc_code'].str[:-1] + '0'\n",
    "# merge the 'test_dfSoc' and 'df_soc' using 'soc_code', only keep the matched sample\n",
    "df_titlelabel = pd.merge(df_titleLabel, df_soc, on='soc_code', how='inner')\n",
    "# keep number of observations by 'soc_code' is more than 100\n",
    "df_titlelabel = df_titlelabel.groupby('soc_code').filter(lambda x: len(x) > 100)\n",
    "# save the final merged DataFrame to a csv file\n",
    "df_titlelabel.to_csv('PATH/df_titleLabel.csv', index=False, encoding = \"utf_8_sig\", header=True, quoting=csv.QUOTE_NONNUMERIC)\n",
    "# randomly select 3000 samples within each unique value of 'soc_code'\n",
    "df = df_titlelabel.groupby('soc_code', group_keys=False).apply(lambda x: x.sample(min(len(x), 3000)))\n",
    "df.to_csv('PATH/secondcheck_sample.csv', index=False, encoding = \"utf_8_sig\", header=True, quoting=csv.QUOTE_NONNUMERIC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Afer first labelling based on the title of job posting, we again use GPT to verify the labelling based on the description of job posting.\n",
    "def classify_job_desp(desp, job_title, api_key):\n",
    "    # The function sends a request to OpenAI's API and returns a yes/no response on the classification accuracy.\n",
    "    url = 'https://api.openai.com/v1/chat/completions'\n",
    "    headers = {'Content-Type': 'application/json',\n",
    "               'Authorization': f'Bearer {api_key}'}\n",
    "    data = {'model': 'gpt-3.5-turbo-0301',\n",
    "            'messages':[\n",
    "                {\n",
    "                'role': 'user', \n",
    "                'content': f\"Based on this job description (in Chinese): '{desp}' Is this Standard Occupational Classification code: '{job_title}' a reasonable classification (at broad group level)? Only tell me yes or no.\"\n",
    "                }\n",
    "                       ]\n",
    "            }\n",
    "    try:\n",
    "        response = requests.post(url, headers=headers, json=data, verify=False)\n",
    "        response_data = json.loads(response.text)\n",
    "        if response.status_code != 200:\n",
    "            raise Exception(response_data['error']['message'])\n",
    "        return response_data['choices'][0]['message']['content']\n",
    "    except Exception as e:\n",
    "        print(f'Error occurred: {e}')\n",
    "        return 'N/A'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Double check on the sub-sampled dataset\n",
    "urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning) # Disable SSL warnings #\n",
    "\n",
    "# Set up parallel processing for validating job descriptions.\n",
    "df = pd.read_csv('PATH/secondcheck_sample.csv', encoding = \"utf_8_sig\", on_bad_lines='skip', encoding_errors='ignore')\n",
    "\n",
    "# Create a list of tuples containing (desp, job_title) pairs\n",
    "desps = df['工作描述'].tolist()\n",
    "job_titles = df['soc_code'].tolist()\n",
    "desp_job_title_pairs = list(zip(desps, job_titles))\n",
    "\n",
    "# Function to handle ThreadPoolExecutor map\n",
    "def classify_wrapper(args):\n",
    "    return classify_job_desp(*args)\n",
    "\n",
    "# Create a thread pool with 30 worker threads\n",
    "with ThreadPoolExecutor(max_workers=20) as executor:\n",
    "    # Submit desp_job_title_pairs to the thread pool\n",
    "    true_inds = list(executor.map(classify_wrapper, [(desp, job_title, api_key) for desp, job_title in desp_job_title_pairs]))\n",
    "\n",
    "# Append the validation results to the DataFrame and clean the data.\n",
    "df['true_ind'] = true_inds\n",
    "# Clean the soc_codes: remove '.' from 'true_ind'\n",
    "df['true_ind'] = df['true_ind'].str.replace('.', '')\n",
    "df.to_csv('PATH/secondcheck_sample_ind.csv', encoding=\"utf_8_sig\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Generate the final dataset for model finetune\n",
    "\n",
    "#  The first screen is filtered based on the job title, and we left with 32 million-ish job postings.\n",
    "#  The second screen is based on the job description. From the 32 million-ish job postings, we randomly sample 3000 observations within each unique SOC category and feed them to GPT for the second check.\n",
    "#  We keep the samples that pass the second check, which means the SOC category predicted by GPT is the same as the SOC category predicted by the job title. This is the first part of the final dataset.\n",
    "#  The second part of the final dataset is the job postings that are first screened by the job title, but has not been selected by the random sampling for the second screen. Within this data pool, we randomly sample 5000 observations within each unique SOC category. This is the second part of the final dataset.\n",
    "#  We combine the two parts of the final dataset, and we assign weight 1 for the second check sample and assign weight 0.5 for the random sample. This is the final dataset for model finetune.\n",
    "\n",
    "df_sample = pd.read_csv('PATH/secondcheck_sample_ind.csv', encoding=\"utf_8_sig\")\n",
    "\n",
    "# keep if true_ind is 'Yes'\n",
    "df_sample = df_sample[df_sample['true_ind'] == 'Yes']\n",
    "\n",
    "# Read another dataset 'df_titleLabel' from the specified CSV file.\n",
    "df_titlelabel = pd.read_csv('PATH/df_titleLabel.csv', encoding = \"utf_8_sig\", on_bad_lines='skip', encoding_errors='ignore')\n",
    "\n",
    "# Merge 'df_sample' and 'df_titlelabel' on '招聘主键ID' using an outer join, and add a merge indicator column.\n",
    "merged_df = df_sample.merge(df_titlelabel, on='招聘主键ID', how='outer', indicator=True)\n",
    "\n",
    "# Select rows from 'merged_df' that are only in 'df_titlelabel' (right_only) and specific columns.\n",
    "filtered_df = merged_df.loc[merged_df['_merge'] == 'right_only', ['招聘主键ID', '工作描述_y', '工作名称_y', 'soc_code_y']]\n",
    "\n",
    "# Rename columns in 'filtered_df' by removing the '_y' suffix added during the merge.\n",
    "filtered_df = filtered_df.rename(columns={'工作描述_y': '工作描述', '工作名称_y': '工作名称', 'soc_code_y': 'soc_code'})\n",
    "\n",
    "# Drop rows from 'filtered_df' where the '工作描述' (job description) column has null values.\n",
    "filtered_df = filtered_df.dropna(subset=['工作描述'])\n",
    "\n",
    "# Within each 'soc_code' group, randomly select up to 5000 samples from 'filtered_df'.\n",
    "filtered_df = filtered_df.groupby('soc_code', group_keys=False).apply(lambda x: x.sample(min(len(x), 5000)))\n",
    "\n",
    "# append 'df_sample' and 'filtered_df', create a new dataframe 'df'\n",
    "df = pd.concat([df_sample, filtered_df], axis = 0)\n",
    "\n",
    "# Keep rows in 'df' where the 'soc_code' does not start with '55'.\n",
    "df = df[~df['soc_code'].str.startswith('55')]\n",
    "df.to_csv('PATH/est_sample.csv', index=False, encoding = \"utf_8_sig\", header=True, quoting=csv.QUOTE_NONNUMERIC)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
