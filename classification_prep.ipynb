{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\DELL\\anaconda3\\lib\\site-packages\\scipy\\__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.2\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import openai\n",
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "from glob import glob\n",
    "from datetime import datetime\n",
    "import gc\n",
    "import requests\n",
    "import json\n",
    "import time\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "openai.organization = \"***\"\n",
    "openai.api_key = \"***\"\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We first load a single dataframe generated from 'data_subset_screen', in which includes all the job postings with title and IDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('E:/Data/job_posting/processed/estimation/charac_posting.csv', encoding = \"utf_8_sig\", on_bad_lines='skip', usecols = ['工作名称'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Then, we clean this dataframe by filtering out the job posting titles with duplicates less than 5 times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count the occurrences of each unique value in the 工作名称 column of a DataFrame df. The result is stored in a new variable called df_counted\n",
    "df_counted = df['工作名称'].value_counts()\n",
    "\n",
    "# Among the 99,315,582 total job postings, there is 35,537,095 job posting titles. \n",
    "df_filtered = df_counted[df_counted>5]\n",
    "df_filtered.shape\n",
    "\n",
    "# Filter to value > 5. This means we need to use ChatGPT to parse 883,695 job posting titles: this is equal to 50,340,840 total job postings\n",
    "df_filtered.sum()\n",
    "\n",
    "# convert 'df_filtered' to a dataframe including a title column and a count column\n",
    "df_filtered = df_filtered.to_frame().reset_index()\n",
    "df_filtered.columns = ['工作名称', 'count']\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parallelize the job title classification using ThreadPoolExecutor, we feed the job titles to `GPT-3.5-turbo` to map it to a SOC category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataframe into 1000 sub-dataframes based on index\n",
    "sub_dfs = np.array_split(df_filtered, 300)\n",
    "\n",
    "# Export each sub-dataframe to a CSV file\n",
    "for i, sub_df in enumerate(sub_dfs):\n",
    "    sub_df.to_csv(f'E:/Data/job_posting/processed/title_raw/title_{i+1}.csv', index=True, encoding = \"utf_8_sig\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_job_title(job_title, api_key):\n",
    "    url = 'https://api.openai.com/v1/chat/completions'\n",
    "    headers = {'Content-Type': 'application/json',\n",
    "               'Authorization': f'Bearer {api_key}'}\n",
    "    data = {'model': 'gpt-3.5-turbo-0301',\n",
    "            'messages':[\n",
    "                {\n",
    "                'role': 'user', \n",
    "                'content': f'The most likely Standard Occupational Classification title and code the occupation fall into: \"{job_title}\", only show the SOC code.'\n",
    "                }\n",
    "                       ]\n",
    "            }\n",
    "    try:\n",
    "        response = requests.post(url, headers=headers, json=data, verify=False)\n",
    "        response_data = json.loads(response.text)\n",
    "        if response.status_code != 200:\n",
    "            raise Exception(response_data['error']['message'])\n",
    "        return response_data['choices'][0]['message']['content']\n",
    "    except Exception as e:\n",
    "        print(f'Error occurred: {e}')\n",
    "        return 'N/A'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parallelize the job title classification using Python's ThreadPoolExecutor\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import requests\n",
    "import json\n",
    "import urllib3\n",
    "\n",
    "# Disable SSL warnings\n",
    "urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\n",
    "\n",
    "os.environ[\"http_proxy\"] = \"http://127.0.0.1:10809\"\n",
    "os.environ[\"https_proxy\"] = \"http://127.0.0.1:10809\"\n",
    "api_key = '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'\n",
    "\n",
    "# create a thread pool with 30 worker threads\n",
    "with ThreadPoolExecutor(max_workers=30) as executor:\n",
    "    # iterate over the file names\n",
    "    for i in range(1, 301):\n",
    "        # read the file into a dataframe\n",
    "        filename = f'E:/Data/job_posting/processed/title_raw/title_{i}.csv'\n",
    "        df = pd.read_csv(filename, encoding=\"utf_8_sig\", on_bad_lines='skip')\n",
    "\n",
    "        # extract the job titles and submit them to the thread pool\n",
    "        job_titles = df['工作名称'].tolist()\n",
    "        futures = [executor.submit(classify_job_title, job_title, api_key) for job_title in job_titles]\n",
    "\n",
    "        # wait for all threads to complete and get the results\n",
    "        # create an empty list to store soc codes\n",
    "        soc_codes = []\n",
    "        for future in futures:\n",
    "            soc_code = future.result()\n",
    "            soc_codes.append(soc_code)\n",
    "\n",
    "        # append the soc codes to the dataframe\n",
    "        df['soc_code'] = soc_codes\n",
    "        # clean the soc codes\n",
    "        df['soc_code'] = df['soc_code'].str.extract(r'(\\d{2}-\\d{4})')\n",
    "        df.to_csv(f'E:/Data/job_posting/processed/title_mapped/title_{i}.csv', encoding=\"utf_8_sig\") \n",
    "        del df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Append all the csv files under folder `E:/Data/job_posting/processed/title_mapped` to a single dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Append all the csv files under folder `E:/Data/job_posting/processed/title_mapped` to a single dataframe\n",
    "path = r'F:/Data/job_posting/processed/title_mapped' # use your path\n",
    "all_files = glob(os.path.join(path, \"*.csv\"))\n",
    "# Append all the csv files in 'all_files' to a single dataframe\n",
    "df_from_each_file = (pd.read_csv(f, encoding=\"utf_8_sig\") for f in all_files)\n",
    "df_title = pd.concat(df_from_each_file, ignore_index=True)\n",
    "# subset dataframe 'df_title' to only keep '工作名称', 'soc_code'\n",
    "df_title = df_title[['工作名称', 'soc_code']]\n",
    "# drop the missing value in column 'soc_code'\n",
    "df_title = df_title.dropna(subset=['soc_code'])\n",
    "df_title"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The next step is to map the rest unmapped job postings to the SOCs using the job descriptions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = 'F:/Data/job_posting/mapped_job_posting/Update file/'\n",
    "\n",
    "# create an empty DataFrame to store merged data\n",
    "df_titleLabel = pd.DataFrame()\n",
    "\n",
    "# iterate over files in that directory\n",
    "for filename in os.listdir(directory):\n",
    "    # checking if it is a file\n",
    "    if filename.startswith(\"job_res_\"): # for files start with a prefix #\n",
    "        f = os.path.join(directory, filename)\n",
    "        df = pd.read_csv(f, encoding = \"utf_8_sig\", on_bad_lines='skip', delimiter= \"?\", encoding_errors='ignore')\n",
    "        df.rename(columns={'招聘ID': '招聘主键ID'}, inplace=True)\n",
    "        df = df[['招聘主键ID', '工作描述', '工作名称']]\n",
    "    # merge 'df_title' with 'df', based on column '工作名称'. Keep the matched row in csv file 'E:/Data/job_posting/processed/finetune/'\n",
    "        df_titleData = pd.merge(df, df_title, on='工作名称', how='inner')# append the merged DataFrame to the empty DataFrame\n",
    "        # change data type of 'soc_code' column to string\n",
    "        # df_titleLabel['soc_code'] = df_titleLabel['soc_code'].astype(str)\n",
    "        df_titleLabel = df_titleLabel.append(df_titleData)\n",
    "        df_titleLabel['soc_code'] = df_titleLabel['soc_code'].astype(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To do this, we first load the dataframe from ONET which contains all the possible SOC job titles. This step helps to remove incorrect mapping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the ONET SOC Code (the real one)\n",
    "df_soc = pd.read_csv('F:/Data/job_posting/processed/2019_to_SOC_Crosswalk.csv')\n",
    "# keep column '2018 SOC Code'\n",
    "df_soc = df_soc[['2018 SOC Code']]\n",
    "# replace the last digit of 'soc_code' with '0'\n",
    "df_soc['2018 SOC Code'] = df_soc['2018 SOC Code'].str[:-1] + '0'\n",
    "# rename the column name to 'soc_code'\n",
    "df_soc.rename(columns={'2018 SOC Code': 'soc_code'}, inplace=True)\n",
    "df_soc = df_soc.drop_duplicates(subset=['soc_code'], keep='first')\n",
    "len(df_soc['soc_code'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We set the finest level to 6 digits, which already covers 459 broad occupations. The concern of going into more granular level is that the number of job postings will be too small to train a good model.\n",
    "\n",
    "- However, not all the broad occupations show up equally in the dataset. We filter out the broad occupations with less than 100 job postings. In this way, it has enough observation to train a good model.\n",
    "- We end up with 408 broad occupations.\n",
    "- we randonmly sample 3000 job postings within each broad occupations, and save them to csv files. We feed this data to ChatGPT to map the job postings to the SOC categories by using job descriptions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace the last digit of 'soc_code' with '0'\n",
    "df_titlelabel['soc_code'] = df_titlelabel['soc_code'].str[:-1] + '0'\n",
    "# merge the 'test_dfSoc' and 'df_soc' using 'soc_code', only keep the matched sample\n",
    "df_titlelabel = pd.merge(df_titlelabel, df_soc, on='soc_code', how='inner')\n",
    "# keep number of observations by 'soc_code' is more than 100\n",
    "df_titlelabel = df_titlelabel.groupby('soc_code').filter(lambda x: len(x) > 100)\n",
    "# save the final merged DataFrame to a csv file\n",
    "df_titlelabel.to_csv('F:/Data/job_posting/processed/finetune/df_titleLabel.csv', index=False, encoding = \"utf_8_sig\", header=True, quoting=csv.QUOTE_NONNUMERIC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "# randomly select 3000 samples within each unique value of 'soc_code'\n",
    "df = df_titlelabel.groupby('soc_code', group_keys=False).apply(lambda x: x.sample(min(len(x), 3000)))\n",
    "df.to_csv('F:/Data/job_posting/processed/finetune/secondcheck_sample.csv', index=False, encoding = \"utf_8_sig\", header=True, quoting=csv.QUOTE_NONNUMERIC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Afer first labelling based on the title of job posting, we again use GPT to verify the labelling based on the description of job posting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_job_desp(desp, job_title, api_key):\n",
    "    url = 'https://api.openai.com/v1/chat/completions'\n",
    "    headers = {'Content-Type': 'application/json',\n",
    "               'Authorization': f'Bearer {api_key}'}\n",
    "    data = {'model': 'gpt-3.5-turbo-0301',\n",
    "            'messages':[\n",
    "                {\n",
    "                'role': 'user', \n",
    "                'content': f\"Based on this job description (in Chinese): '{desp}' Is this Standard Occupational Classification code: '{job_title}' a reasonable classification (at broad group level)? Only tell me yes or no.\"\n",
    "                }\n",
    "                       ]\n",
    "            }\n",
    "    try:\n",
    "        response = requests.post(url, headers=headers, json=data, verify=False)\n",
    "        response_data = json.loads(response.text)\n",
    "        if response.status_code != 200:\n",
    "            raise Exception(response_data['error']['message'])\n",
    "        return response_data['choices'][0]['message']['content']\n",
    "    except Exception as e:\n",
    "        print(f'Error occurred: {e}')\n",
    "        return 'N/A'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Double check on the sub-sampled dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parallelize the job title classification using Python's ThreadPoolExecutor\n",
    "import os\n",
    "import pandas as pd\n",
    "import requests\n",
    "import json\n",
    "import urllib3\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "# Disable SSL warnings\n",
    "urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\n",
    "\n",
    "os.environ[\"http_proxy\"] = \"http://127.0.0.1:10809\"\n",
    "os.environ[\"https_proxy\"] = \"http://127.0.0.1:10809\"\n",
    "api_key = '***'\n",
    "\n",
    "# Read the file into a dataframe, start from the 3000th row\n",
    "df = pd.read_csv('F:/Data/job_posting/processed/finetune/secondcheck_sample.csv', encoding = \"utf_8_sig\", on_bad_lines='skip', encoding_errors='ignore')\n",
    "\n",
    "# Create a list of tuples containing (desp, job_title) pairs\n",
    "desps = df['工作描述'].tolist()\n",
    "job_titles = df['soc_code'].tolist()\n",
    "desp_job_title_pairs = list(zip(desps, job_titles))\n",
    "\n",
    "# Function to handle ThreadPoolExecutor map\n",
    "def classify_wrapper(args):\n",
    "    return classify_job_desp(*args)\n",
    "\n",
    "# Create a thread pool with 30 worker threads\n",
    "with ThreadPoolExecutor(max_workers=20) as executor:\n",
    "    # Submit desp_job_title_pairs to the thread pool\n",
    "    true_inds = list(executor.map(classify_wrapper, [(desp, job_title, api_key) for desp, job_title in desp_job_title_pairs]))\n",
    "\n",
    "# Append the soc_codes to the dataframe\n",
    "df['true_ind'] = true_inds\n",
    "\n",
    "# Clean the soc_codes\n",
    "# remove '.' from 'true_ind'\n",
    "df['true_ind'] = df['true_ind'].str.replace('.', '')\n",
    "df.to_csv('F:/Data/job_posting/processed/finetune/secondcheck_sample_ind.csv', encoding=\"utf_8_sig\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate the final dataset for model finetune\n",
    "\n",
    "- The first screen is filtered based on the job title, and we left with 32 million-ish job postings.\n",
    "- The second screen is based on the job description. From the 32 million-ish job postings, we randomly sample 3000 observations within each unique SOC category and feed them to `GPT-3.5-turbo` for the second check.\n",
    "- We keep the samples that pass the second check, which means the SOC category predicted by GPT is the same as the SOC category predicted by the job title. This is the first part of the final dataset.\n",
    "- The second part of the final dataset is the job postings that are first screened by the job title, but has not been selected by the random sampling for the second screen. Within this data pool, we randomly sample 5000 observations within each unique SOC category. This is the second part of the final dataset.\n",
    "- We combine the two parts to form the final dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "df_sample = pd.read_csv('F:/Data/job_posting/processed/finetune/secondcheck_sample_ind.csv', encoding=\"utf_8_sig\")\n",
    "\n",
    "# keep if true_ind is 'Yes'\n",
    "df_sample = df_sample[df_sample['true_ind'] == 'Yes']\n",
    "\n",
    "# create into train and test set\n",
    "df_titlelabel = pd.read_csv('F:/Data/job_posting/processed/finetune/df_titleLabel.csv', encoding = \"utf_8_sig\", on_bad_lines='skip', encoding_errors='ignore')\n",
    "\n",
    "# merge the dataframes on '招聘主键ID'\n",
    "merged_df = df_sample.merge(df_titlelabel, on='招聘主键ID', how='outer', indicator=True)\n",
    "\n",
    "# filter the merged dataframe to keep only the observations in 'df_titlelabel' but not in 'df_sample'\n",
    "# merged_df.reset_index(inplace=True)\n",
    "filtered_df = merged_df.loc[merged_df['_merge'] == 'right_only', ['招聘主键ID', '工作描述_y', '工作名称_y', 'soc_code_y']]\n",
    "\n",
    "# note that df_titlelabel.columns is used to select only the columns from 'df_titlelabel'\n",
    "# rename columns by removing '_y'\n",
    "filtered_df = filtered_df.rename(columns={'工作描述_y': '工作描述', '工作名称_y': '工作名称', 'soc_code_y': 'soc_code'})\n",
    "\n",
    "# drop if '描述' is null\n",
    "filtered_df = filtered_df.dropna(subset=['工作描述'])\n",
    "\n",
    "# randomly select 3000 samples within each unique value of 'soc_code'\n",
    "filtered_df = filtered_df.groupby('soc_code', group_keys=False).apply(lambda x: x.sample(min(len(x), 5000)))\n",
    "\n",
    "# append 'df_sample' and 'filtered_df', create a new dataframe 'df'\n",
    "df = pd.concat([df_sample, filtered_df], axis = 0)\n",
    "\n",
    "# drop if the first two digits in 'soc_code' are '55', which is a SOC code that is highly unlikely to hire from online job postings\n",
    "df = df[~df['soc_code'].str.startswith('55')]\n",
    "\n",
    "df.to_csv('F:/Data/job_posting/processed/finetune/est_sample.csv', index=False, encoding = \"utf_8_sig\", header=True, quoting=csv.QUOTE_NONNUMERIC)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
