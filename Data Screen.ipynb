{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This script processes job posting data from various sources. It reads data files, cleans and filters the data based on specific criteria (like removing duplicates and filtering by source), and finally extracts and saves job titles and descriptions for further analysis. Requires input data in .dat format and outputs processed data in CSV format.\n",
    "\n",
    "# -*- coding: utf-8 -*-\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import gc, json, csv, re, os, glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_dat_data(curFile):\n",
    "    \"\"\"\n",
    "    Reads a .dat file containing job posting data and returns a DataFrame.\n",
    "    The function expects a specific format with predefined column names and uses '@!' as a separator.\n",
    "    Parameters:\n",
    "    curFile (str): File path of the .dat file to be read.\n",
    "    Returns:\n",
    "    pandas.DataFrame: Contains the job posting data with specified column names.\n",
    "    \"\"\"\n",
    "    colNames = ['招聘主键ID','公司ID','公司名称','城市名称','公司所在区域','工作薪酬','教育要求','工作经历',\n",
    "                '工作描述','职位名称','工作名称','招聘数量','发布日期','行业名称','数据来源']\n",
    "    resCSV = pd.read_csv(curFile, header=None, index_col=None, names=colNames,encoding='utf-8',quoting=csv.QUOTE_NONE, sep=\"@!\", error_bad_lines=False, engine='python')\n",
    "    return resCSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start\n",
      "2021-10-25 10:57:41\n"
     ]
    }
   ],
   "source": [
    "# Data Cleaning and Preparation Steps:\n",
    "# Step 1: Replace missing job titles ('工作名称') with position names ('职位名称').\n",
    "# Step 2: Remove entries where the publication date ('发布日期') is missing.\n",
    "# Step 3: Exclude part-time jobs ('兼职') from the dataset.\n",
    "# Step 4: Eliminate duplicates within a month considering '公司ID', '工作名称', '城市名称' as identifying fields.\n",
    "# Step 5: Retain job postings from major websites only, based on the '数据来源' field.\n",
    "\n",
    "dataNameTmp = \"PATH/job_posting_%s.dat\"\n",
    "\n",
    "naSum = 0\n",
    "dupSum = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2c8efa0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1,142): \n",
    "    # Process each job posting file: Clean, filter, and save the processed data.\n",
    "\n",
    "    curFile = dataNameTmp%i\n",
    "        \n",
    "    print(curFile,\" is Grouping Computing...\")\n",
    "    datDf = read_dat_data(curFile)\n",
    "    datDf = datDf.replace(r'\\N',np.NaN).dropna(subset=['发布日期'])\n",
    "    datDf['工作名称'] = datDf[['工作名称']].replace(r'\\N',np.NaN)\n",
    "    datDf.loc[datDf['工作名称'].isna(),'工作名称'] = datDf.loc[datDf['工作名称'].isna(),'职位名称']\n",
    "    datDf = datDf[datDf['工作名称'] != \"兼职\"]\n",
    "     # subset the data to only include the '来源' == '智联招聘', '前程无忧', '拉勾网', 'BOSS直聘', '58同城', '猎聘网', '看准网', 百姓网', '拉勾网', '猎聘', '赶集网'， 'BOSS'\n",
    "    datDf = datDf[datDf['数据来源'].isin(['智联招聘', '前程无忧', '拉勾网', 'BOSS直聘', '58同城', '猎聘网', '看准网', '百姓网', '拉勾网', '猎聘', '赶集网', 'BOSS'])]\n",
    "\n",
    "    curNa = datDf.shape[0]\n",
    "    naSum = naSum + curNa\n",
    "    \n",
    "    datDf['date'] = datDf['发布日期'].apply(lambda x: x[0:7])\n",
    "    datDf = datDf.drop_duplicates(subset=['公司ID', '工作名称', '城市名称', 'date'], keep='first').reset_index(drop=True)\n",
    "    \n",
    "    curDup = datDf.shape[0]\n",
    "    dupSum = dupSum + curDup\n",
    "    \n",
    "    print(curFile,\"删除空值剩余: %s\"%curNa, \"去重复值剩余：%s\"%curDup)\n",
    "    datDf.to_csv('PATH/job_res_{}.csv'.format(i), sep='?', encoding = 'utf_8_sig', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c26b6b34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# From the master data, we separate ``job description\" so that rest of the data are manageable. \n",
    "\n",
    "directory = 'PATH/'\n",
    "\n",
    "# iterate over files in that directory\n",
    "for filename in os.listdir(directory):\n",
    "    # checking if it is a file\n",
    "    if filename.startswith(\"job_res_\"): # for files start with a prefix #\n",
    "        f = os.path.join(directory, filename)\n",
    "        df = pd.read_csv(f, encoding = \"utf_8_sig\", on_bad_lines='skip', delimiter= \"?\", header=None, encoding_errors='ignore')\n",
    "        df.rename(columns={0: '招聘主键ID', 1: '公司ID', 2: '公司名称', 3: '城市名称', 4: '公司所在区域', 5: '工作薪酬', 6: '教育要求', \n",
    "                   7: '工作经历', 8: '工作描述', 9: '职位名称', 10: '工作名称', 11: '招聘数量', 12: '发布日期', 13: '行业名称', \n",
    "                   14: '数据来源'}, inplace=True)\n",
    "        df_charac = df[['招聘主键ID', '公司ID', '公司名称', '城市名称', '公司所在区域', '工作薪酬', '教育要求', '工作经历', '职位名称', \n",
    "         '工作名称', '招聘数量', '发布日期', '行业名称', '数据来源']]\n",
    "        \n",
    "        # export the data to csv, use the header and set the encoding to utf-8\n",
    "        df_charac.to_csv('PATH/{}'.format(filename), encoding = \"utf_8_sig\", header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d86b4fff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Append all the character data together, then generate a list of the job titles that used to feed to the ChatGPT\n",
    "os.chdir(\"PATH\")\n",
    "extension = 'csv'\n",
    "all_filenames = [i for i in glob.glob('*.{}'.format(extension))]\n",
    "#combine all files in the list\n",
    "combined_csv = pd.concat([pd.read_csv(f, encoding = \"utf_8_sig\", on_bad_lines='skip', usecols = ['工作名称']) for f in all_filenames], ignore_index=True)\n",
    "# This is the complete list of job posting titles \n",
    "combined_csv.to_csv('PATH/charac_posting.csv', index=False, header=True)\n",
    "# Save all the ``job description\" data\n",
    "\n",
    "directory = 'PATH/'\n",
    "# iterate over files in that directory\n",
    "for filename in os.listdir(directory):\n",
    "    # checking if it is a file\n",
    "    if filename.startswith(\"job_res_\"): # for files start with a prefix #\n",
    "        f = os.path.join(directory, filename)\n",
    "        df = pd.read_csv(f, encoding = \"utf_8_sig\", on_bad_lines='skip', delimiter= \"?\", header=None, encoding_errors='ignore')\n",
    "        df.rename(columns={0: '招聘主键ID', 1: '公司ID', 2: '公司名称', 3: '城市名称', 4: '公司所在区域', 5: '工作薪酬', 6: '教育要求', \n",
    "                   7: '工作经历', 8: '工作描述', 9: '职位名称', 10: '工作名称', 11: '招聘数量', 12: '发布日期', 13: '行业名称', \n",
    "                   14: '数据来源'}, inplace=True)\n",
    "        df_desp = df[['招聘主键ID', '公司ID', '工作描述']]\n",
    "        df_desp.to_csv('E:/Data/job_posting/processed/description/{}'.format(filename))\n",
    "        \n",
    "        del df_desp\n",
    "        del df\n",
    "        gc.collect()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5b71bd63",
   "metadata": {},
   "source": [
    "### Note: \n",
    "\n",
    "- The cell below is used to determine the website source, it has been determined and does not need to be run again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eea5509",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine the data source, we limit to Top 10 job posting websites to avoid fuzzywuzzy in the data source.\n",
    "dataNameTmp = \"PATH/job_posting_%s.dat\"\n",
    "\n",
    "df_list = []\n",
    "\n",
    "for i in range(1,3): \n",
    "    curFile = dataNameTmp%i\n",
    "        \n",
    "    print(curFile,\" is Grouping Computing...\")\n",
    "    datDf = read_dat_data(curFile)\n",
    "\n",
    "\n",
    "    # count number of occurrences of each value in column '数据来源', generate a new column to record the count\n",
    "    datDf['count'] = datDf.groupby('数据来源')['数据来源'].transform('count')\n",
    "\n",
    "    # drop duplicates based on column '数据来源', only keep the first occurrence\n",
    "    datDf = datDf.drop_duplicates(subset=['数据来源'], keep='first').reset_index(drop=True)\n",
    "    datDf = datDf[['数据来源', 'count']]\n",
    "\n",
    "    df_list.append(datDf)\n",
    "        \n",
    "final_df = pd.concat(df_list)\n",
    "\n",
    "# group by '数据来源' and sum the count\n",
    "final_df = final_df.groupby('数据来源').sum().reset_index()\n",
    "\n",
    "# sort the dataframe based on column 'count'\n",
    "final_df = final_df.sort_values(by=['count'], ascending=False)\n",
    "final_df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e007b1bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
